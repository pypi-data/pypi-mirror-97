# General testing and feedback

Unified tester for any language, for any exam, using Gradescope Autograder

## Currently supported languages

 - Python
 - C++

## How it works

This framework is designed to run a set of tests on student submitted code. This is achieved with a combination of built-in tests and using specific scripts to test certain aspects of the functionality of the student's code. These are called testers.

The set of tests are configured in a configuration file that also sets the test parameters and score awarded for each test.

As an output a result.json file is generated that is compatible with the format Autograder expects.

## How to run

run the feedback module with the name of the tested file, and a [configuration file](##Config-file-arhitecture)

The framework assumes that all required files are in the same directory as the tested file. The all files in the directory containing the tested file will be copied, and used for evaluation

When the module is installed with pip, the command `feedback` is added, to run the module directly.

### Run parameters

 - `-v` - Verbose execution
 - `-f [PATH]` - Specify the location of the result file
 - `-x` - Cleare the cache of compiled files. Only run this if you think this is impacting your tests. This will slow down test execution
 - `-c [PATH]` - Copy the temporary directory where evaluation took place to the specified path after evaluation completed. Used to debug test execution
 - `-d` - Disable result validation. Result files generated by testers are only deemed valid, if the appropriate validation logic is followed. If you're not using one of the tester templates and don't want to include validation, enable this option. Carefull, this enables the student to maliciously modify results.

### Examples

Python:
```sh
python -m feedback "tests/fixtures/integration/py/tested.py" "tests/fixtures/integration/py/py_config.json"
```

C++:
```sh
python -m feedback "tests/fixtures/integration/cpp/tested.cpp" "tests/fixtures/integration/cpp/cpp_config.json"
```

Deployed:
```sh
feedback "tests/fixtures/integration/cpp/tested.cpp" "tests/fixtures/integration/cpp/cpp_config.json"
```

## Config file arhitecture

The config file uses JSON to configure test runs

### Tests

Specify a list of tests that should be ran, as an array under the `"tests"` key, with each having `type` to identify the test, and a set of parameters with values. Tests are executed in the order specified in the config file

Every test requires the `"max_score"` key, and some tests may require other parameters

#### Currently supported tests and parameters
 - Parameters supported by all tests
    - **max_score** - this parameter is required on every tests, unless explicitly stated. This sets the total maximum score for that test. 0 is allowed, where the test doesn't count towards the score, but can provide feedback towards the student.
    - (optional) **number** - assign a number to the test. If the test run generates multiple results, these will be added to this number separated by a dot
    - (optional) **tags** - tags are supported by Autograder and will be passed through directly
    - (optional) **visibility** - visiblity controll for Autograder. Will be passed through directly
 - Parameters supported by *execution tests*
    - **timeout**, **timeout_sec**, **timeout_min**, **timeout_ms** - maximum duration after which execution is interrupted and terminated. If set to -1 timeout will be disabled (this doesn't disable the default autograder timeout. Disabling the timeout is not recommended). Default is 30 seconds. Default unit is seconds
    - **child_process_limit** - maximum number of child processes the tested code is allowed to start. If more than the allowed number of processes are detected, execution is terminated. Default is 0. Outside very specific cases it is not recommended to enable the tested code to start child processes. This limit accounts for the additional process when [executable](###Executable) testing is used. Child limit can be disabled by setting this value to -1
    - **memory_limit**, **memory_limit_kb**, **memory_limit_mb** - Limit the maximum memory allocated by the tested file. This can be used to check for memory leaks in C++ testing. If more than the allowed memory is allocated execution is terminated. Default is 526mb. Default unit is bytes. Memory limit can be disabled by setting it to -1. If multiple memory limits is specified, the smallest is used.
    - **allow_connections** - Allow or forbid the tested file to start any network connections. If it's not allowed and the tested file attempts to start a connection, execution is terminated. Allowed values: `true`, `false`, default: `false`
- Tests supported for all languages:
    - **replace_files** - replace files with default versions. Files matched by name. Commonly used to ensure `.h` file are not modified. This test doesn't support **max_score** config value
        - **files** - path to the files to use for replacement.
    - **comments** - evaluate the comment density of the tested file. See [how comment scoring works](##Comment-scoring)

 - C++
    - **compile** - compile submitted C++ file
        - (optional) **warning_penalty** [0.0, 1.0] - what proportion of the max score is deducted per each warning generated per comilation warning. Default is 0.2
    - **functionality** - test the functionality with a [compiled-in tester](###Compiled-in)
        - **tester_file** - path to the tester file
        - Supports all *execution test* parameters
    - **functionality_executable** - test the functionailty with an [executable tester](###Executable)
        - **tester_file** - path to the tester file
        - Supports all *execution test* parameters
    - **static** - run static analysis on the tested file using `cppcheck`
        - (optional) **error_penalty** [0.0, 1.0] - what proportion of the max score is deducted per each error. Default is 0.2
    - **style** - run code style check using `clang-format`
        - **style** - the target style. Default is `google`
 - Python
    - **syntax** - try to load the python tested file and check for syntax issues. If the module loads, full score is awarded
    - **functionality** - test the functionality of the tested file. For details see [how python execution works](###Python)
        - **tester_file** - path to the tester file
        - Supports all *execution test* parameters
    - **static** - run static analysis on the tested file, using lint
        - (optional) **error_penalty** [0.0, 1.0] - what proportion of the max score is deducted per static error. Default is 1
        - (optional) **warning_penalty** [0.0, 1.0] - what proportion of the max score is deducted per static warning. Default is 0.1
        - (optional) **convention_penalty** [0.0, 1.0] - what proportion of the max score is deducted per convention violation. Default is 0.05
        - (optional) **refactor_penalty** [0.0, 1.0] - what proportion of the max score is deducted per suggested refactoring. Default is 0.05

### Global configuration

Some parameters must be applied to the entire evaluation. These are defined in a separate `"globals"` section after the `"tests"` section.

#### Supported global parameters
 - **cpp_std** - Specify the C++ standard that's used by the compiler. Default is `c++11`. From [full list of standards](https://man7.org/linux/man-pages/man1/g++.1.html) versions above `c++98` are supported. `gnu++` standards are supported above `gnu++98`. Depricated naming convenstions are not supported.


### Example config

```
{
    "tests": [
        {
            "type": "compile",
            "max_score": 10.0,
            "number": "1",
            "tags": [],
            "visibility": "visible"
        },
        {
            "type": "functionality",
            "max_score": 60.0,
            "number": "2",
            "tags": [],
            "visibility": "visible",
            "tester_file": "run_code.cpp"
        },
        {
            "type": "static",
            "max_score": 10.0,
            "number": "3",
            "tags": [],
            "visibility": "visible"
        },
        {
            "type": "comments",
            "max_score": 10.0,
            "number": "4",
            "tags": [],
            "visibility": "visible"
        },
        {
            "type": "style",
            "max_score": 10.0,
            "number": "5",
            "tags": [],
            "visibility": "visible",
            "style": "google"
        }
    ]
}
```

## Methods of testing

The following methods of tested file execution are supported:

### Compiled-in

For module-like C++ files, where the API is defined in a `.h` file, but the tested file has no individual execution component, complied-in execution should be used, where a tester, using the following template, imports the module and runs the various functions/classes defined. The template described here takes care of skipping and validation.

```cpp
#include "TESTED_MODULE.h"
#include "cpp_eval_util.h"  // Module containing the evaluator prototype

class NAMEEvaluator : public Evaluator<TYPE>{
  public:
    
    NAMEEvaluator(int argc, char** argv):Evaluator(argc, argv){}

    // For a given question i return the name of the question
    // This is used to discover the number of tests.
    // If i is greater than or equal to the number of tests, return an empty string "".
    // This indicates that the limit has been reached.
    string GetName(int i){
      return i < no_of_questions ? "Name" : ""; // Example only
    }

    // For a given question i return the value/object evaluated for the given test
    TYPE GetResult(int i){
      return 
    }

    // For a given question i, which returned result return the score that can be rewarded for the result
    float GetScore(int i, TYPE result){
      return result == expected ? 1.0f : 0.0f; // Example only
    }

    // For a given question i, which returned result and was awarded score, return appropriate feedback
    string GetFeedback(int i, TYPE result, float score){
      return "Appropriate feedback" + (score >= 1 ? " : PASS!" : " : FAIL!");   // Example only
    }
};

int main(int argc, char **argv) {
  NAMEEvaluator evaluator(argc, argv);
  return evaluator.Run();
}
```
Replace NAME and TYPE to the name of the test and the return TYPE from the tested function/class

### Executable

For executable-like C++ files, where the file executed by itself prints some output. The tester, using the following template, can run the file, enter inputs and check outputs. The template described here takes care of the skipping and validation.

```py
from py_eval_util import Evaluator  # Py_eval_util will always be available.

def test_file():    

    # Create evaluator object
    e = Evaluator()

    # For a given question i return the name of the question
    # This is used to discover the number of tests.
    # If i is greater than or equal to the number of tests, return None.
    # This indicates that the limit has been reached.
    e.with_name(lambda i: "Name" if i < no_test_cases else None)  # Example only
    
    # Specify that you want to treat the tested file as an executable
    e.run_executable()
    
    # For a given question i pass in the following input to the executable on stdin, supports single value, and iterable.
    e.with_input(lambda i: )

    # For a given question i which returned result lines, return a score that can be rewarded for it
    # Result is a string list
    e.with_score(lambda i, result: )

    # For a given question i which returned result lines and was awarded score return some appropriate feedback
    e.with_feedback(lambda i, result, score: "Appropriate feedback : " + ("PASS" if score >= 1 else "FAIL"))    # Example only

    # Start evaluation
    e.start()

if __name__ == "__main__":
    test_file()
```
The evaluator object also supports `e.with_score_and_feedback(lambda i, result: 0, "feedback")` which must return the score and the feedback a single tuple

The usage of lambda function is not required, it is also allowed to pass in the name of a function to the builder:
```py

def score_provider(i, result):
    return 0    # Some score

def test_file():
    ...
    e.with_score(score_provider)
```

### Python

For python tested files both module and executable testing is handled in a similar fashion. Use the following templates for the various approaches:

Test individual function or class:
```py
from py_eval_util import Evaluator

def test_file():

    # Expected signature (the name and arguments will be matched with a function in the tested module)
    def sig_fun(x, y):
        pass

    # Alternatively, a class can be found by it's name and init signature
    # Keep in mind, only the init signature is matched, not other methods/properties.
    # Make sure any method/property exists before accessing it using the hasattr function
    class SigClass:
        def __init__(self, x, y):
            pass

    # Create evaluator object
    e = Evaluator()

    # For a given question i return the name of the question
    # This is used to discover the number of tests.
    # If i is greater than or equal to the number of tests, return None.
    # This indicates that the limit has been reached.
    e.with_name(lambda i: "Name" if i < [no_test_cases] else None)  # Example only

    # For a given question i, and a function from the module matching the requested signature, return some value that will be evaluated
    e.run_code(lambda i, f: f(...), signature=sig_fun)

    # For a given question i which returned result lines, return a score that can be rewarded for it
    e.with_score(lambda i, result: )
    
    # For a given question i which returned result lines and was awarded score return some appropriate feedback
    e.with_feedback(lambda i, result, score: "Appropriate feedback : " + ("PASS" if score >= 1 else "FAIL"))    # Example only

    # Start evaluation
    e.start()


if __name__ == "__main__":
    test_file()

```

Evaluator also supports `with_score_and_feedback` and full functions instead of lambdas (see above)

Test whole modules that are loaded as main, and will execute some code automatically:
```py
from py_eval_util import Evaluator


def test_file():

    # Create evaluator object
    e = Evaluator()

    # For a given question i return the name of the question
    # This is used to discover the number of tests.
    # If i is greater than or equal to the number of tests, return None.
    # This indicates that the limit has been reached.
    e.with_name(lambda i: "Name" if i < [no_test_cases] else None)  # Example only

    # Run the tested module as main
    e.run_module()

    # For a given question i pass in the following input to the module on stdin, supports single value, and iterable.
    e.with_input(lambda i: )

    # For a given question i which printed result lines, return a score that can be rewarded for it
    # Result is a string list
    e.with_score(lambda i, result: )
    
    # For a given question i which returned result lines and was awarded score return some appropriate feedback
    e.with_feedback(lambda i, result, score: "Appropriate feedback : " + ("PASS" if score >= 1 else "FAIL"))    # Example only
    
    # Start evaluation
    e.start()


if __name__ == "__main__":
    test_file()
```
Evaluator also supports `with_score_and_feedback` and full functions instead of lambdas (see above)

## Py Eval util

The py_eval_util module, that is available for all python testers, have the following usefull functions:

 - `get_module_functions(module)` - returns all function in a module
 - `get_module_classes(module)` - returns all classes in a module
 - `get_levenshtein_ratio(string1, string2)` - determin how similar two strings are to each other
 - `result_to_dictionary(question, mark, weight, feedback)` - turn question result into a dictionary, with correct names and rounded marks
 - `numbers_close(a, b, margin)` - determin if two numbers (a, b) are within margin to each other
 - `InputOverride(list)` - object that overrides stdin with the passed in list, and cleans up after itself when disposed. Use with the `with` keyword. If stdin is accessed too many times the `too_many_reads` flag is set to `true`
 - `Capturing()` - object that captures everything from stdout. It must be used with the `with` keyword. Results are stored in the object as a list
 - `CapturingErr()` - object that captures everything from stderr. It must be used with the `with` keyword. Results are stored in the object as a list

## Comment scoring

The score for comment tests are calculated in the following way:

 - The number of code lines and comment lines are counted with `cloc`. Empty lines are ignored
 - Comment density is calculated as such: comment lines / (comment lines + code lines)
 - The base of the score is the comment density
 - The comment density is matched with one of the categories in [comment_feedback.json](.\feedback\lookup\comment_feedback.json) by selecting the largest category it's smaller than. If that category has a bonus, that bonus is added.

## Feedback builder

To simplify creating the zip file that is uploaded to autograder, and to ensure that the setup file is configured correctly, use the feedback-builder feature. It's given the path to the tested file and the config file, and it will collect all files mentioned in the config, insert the default `setup` and `run_autograder` files and modify file paths in the copied config to use the autograder convention. It then outputs all files in a zip file that can be uploaded to gradescipe.

`feedback-builder` is available as a command after the `UoL-Autograder` package is installed with pip.

It is not recommended to build your own zip file with your own setup script. The setup script in the builder is kept up to date to ensure the secure testing of student code.

It should be used with pip. It's not recommended to run the builder in development mode.

### Run parameters

 - `-v` - Verbose execution
 - `-o` - Specify the output zip path (including name)

### Example

```sh
feedback-builder "tests/fixtures/integration/cpp/tested.cpp" "tests/fixtures/integration/cpp/cpp_config.json"
```

```sh
feedback-builder "tests/fixtures/integration/cpp/tested.cpp" "tests/fixtures/integration/cpp/cpp_config.json" -o "output/path/integration.zip"
```

## Creating your own tester framework

DO NOT DO THIS, UNLESS YOU KNOW WHAT YOU ARE DOING!

The is passed a set of run arguments that govern how it runs. These are in order:
```sh
[OUTPUT_PATH] [SKIP_NUMBER] [SECRET_LOCATION]
```

There are 2 crucial concepts that the tester must implement: skipping and validation.
 - skipping is the ability to stich togeter results from multiple runs, if a question fails. To do this the tester must be able to skip a certain number of questions
 - validation is the method of assuring that the result was not modified by the student. This requires the generation of a validation file.

The tester must create a file to the [OUTPUT_PATH] location, with the result of the test encoded in a JSON format. At the start of the test, the tester must create the result file, with the appropriate number of results, question names and weights, but the marks and feedbacks left blank (`""`). These must be populated one by one, as the questions are executed. If the [SKIP_NUMBER] is specified, the first [SKIP_NUMBER] of questions must be skipped and not populated, but the feedbacks file must still have results for all questions, and leave the skipped ones blank. This is why it's important to have questions that can be executed independent of each other.

Each time the result file is update, a validation file must be created alongside it. It must be located in the same directory as the result file and it must be called [SECRET_LOCATION]. In order to generate the validation file the secret key must be obtained. The key is initially located in an environment variable called [SECRET_LOCATION]. Before executing any student code the variable must be read and replaced with an empty string. Student code is not allowed to access the key. The validation file must contain the sha256 hash of the result file and the secret key (combined in that order). If the validation file is missing, or it doesn't contain the expeted hash, the question result will not be considered, unless validation is disabled.


# Running it in docker locally

In the current directory build the docker image used for testing:
```sh
docker build --pull --rm -f "Dockerfile" -t feedback-dev:latest .
```

Once the build succeeds start the container in interactive mode and mount the current folder

```sh
docker run --rm --name feedback -v ${PWD}:/app -it feedback-dev:latest bash
```

In the `/app` location, run tests by typing `pytest` or run the framework.
