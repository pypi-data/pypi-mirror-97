{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How tos\n",
    "\n",
    "Here we list a few common use cases and how you can use this package to address these use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to install `aermanager`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal and navigate to your favorite software repository location.\n",
    "\n",
    "```\n",
    "$ pip install aermanager\n",
    "```\n",
    "\n",
    "Then in a python terminal you should be able to import aermanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aermanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change directory in case you are running this notebook from the original location in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sadique/git/aermanager\n"
     ]
    }
   ],
   "source": [
    "cd ../../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load events from an *.aedat4 file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the sensor recording the data was inferred to be (240, 320)\n"
     ]
    }
   ],
   "source": [
    "from aermanager.aerparser import load_events_from_file\n",
    "from aermanager.parsers import parse_aedat4\n",
    "\n",
    "filename = \"test/data/class2/data_sample.aedat4\"\n",
    "shape, events = load_events_from_file(filename, parser=parse_aedat4)\n",
    "print(f\"The shape of the sensor recording the data was inferred to be {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(152, 126, 1605116853975736,  True),\n",
       "       (148,  79, 1605116853975738, False),\n",
       "       (146,  77, 1605116853975740, False), ...,\n",
       "       ( 74, 227, 1605116855435678,  True),\n",
       "       ( 42, 157, 1605116855435698, False),\n",
       "       ( 41, 216, 1605116855435698, False)],\n",
       "      dtype=[('x', '<u2'), ('y', '<u2'), ('t', '<u8'), ('p', '?')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load events from an *.aedat file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will still use the `load_events_from_file` method but you will need to specify the `parser`. You can use the following inbuilt parsers for the corresponding sensors: \n",
    "1. 'DVS128': use `parse_dvs128` (128, 128)\n",
    "2. 'mini346': use `parse_346mini` (260, 346)\n",
    "3. 'redV3': use `parse_red` (132,104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to slice events ?\n",
    "\n",
    "An event list extracted from a file could contain a very large sequence of events. Typically such a long spike list is not directly usable. \n",
    "\n",
    "For instance, if we are trying to train a network on such data, we typically want to present a network a series of images, where each image corresponds to a small slice of events. These smaller slices are generated typically using one of two criterion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Slice by spike count\n",
    "\n",
    "Here we define a slice as comprising of a fixed number of events (irrespective of the time length of each such slice). This strategy enables a way of auto-normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from aermanager.preprocess import slice_by_count\n",
    "\n",
    "# Generate indices to split the data every for 3000 spikes\n",
    "indices = slice_by_count(events, spike_count=3000)\n",
    "\n",
    "# Slice the data\n",
    "sliced_events = np.split(events, indices)[:-1]\n",
    "\n",
    "## Number of slices\n",
    "len(sliced_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3000,), dtype([('x', '<u2'), ('y', '<u2'), ('t', '<u8'), ('p', '?')]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_events[0].shape, sliced_events[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Slice by time window\n",
    "\n",
    "Here we define a slice as comprising of all events within a given time-window. This strategy ensures that the data has fixed time bounds and could be used to avoid blurring effects when creating frames or for learning spatio-temporal features when including time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aermanager.preprocess import slice_by_time\n",
    "\n",
    "# Generate indices to split the data at every 3ms (3000 us)\n",
    "indices = slice_by_time(events, time_window=3000)\n",
    "\n",
    "# Slice the data\n",
    "sliced_events = np.split(events, indices)[:-1]\n",
    "\n",
    "## Number of slices\n",
    "len(sliced_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate frames from events ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(486, 2, 240, 320)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aermanager.preprocess import accumulate_frames\n",
    "\n",
    "frames = accumulate_frames(\n",
    "    sliced_events, \n",
    "    bins_y=range(shape[0]+1), \n",
    "    bins_x=range(shape[1]+1))\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate spike-rasters from events ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike raster has the shape: (3, 2, 240, 320)\n",
      "The number of events in the raster 674.0 should match total events 674\n"
     ]
    }
   ],
   "source": [
    "from aermanager.preprocess import create_raster_from_xytp\n",
    "\n",
    "\n",
    "# Let us consider the first slice we created above\n",
    "events_xytp = sliced_events[0]\n",
    "\n",
    "# dt is the time step we will use to rasterize.\n",
    "dt = 1000  #ms\n",
    "\n",
    "# Convert this set of events to a raster\n",
    "spike_raster = create_raster_from_xytp(\n",
    "    events_xytp, \n",
    "    dt=dt, \n",
    "    bins_y=range(shape[0]+1), \n",
    "    bins_x=range(shape[1]+1))\n",
    "\n",
    "# The spike raster should be of shape [time, polarity, height, width]\n",
    "print(f\"Spike raster has the shape: {spike_raster.shape}\")\n",
    "\n",
    "# The total number of spikes in this slice should be equal to the sum of all the values in the spike_raster\n",
    "print(f\"The number of events in the raster {spike_raster.sum()} should match total events {len(events_xytp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spike raster contains number of spikes produced in a given time window by any given neuron of a given polarity. We see that the slice we generated above had a lenth of 3 time steps of $1ms$ each. \n",
    "\n",
    "This format of data is useful as data input to `sinabs` models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to crop events ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 250)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aermanager.preprocess import crop_events\n",
    "\n",
    "left, right = 35,35\n",
    "top, bottom = 4, 4\n",
    "\n",
    "events_cropped, new_shape = crop_events(\n",
    "    events, \n",
    "    input_shape=shape, \n",
    "    crop_size=((left, right), (top, bottom)))\n",
    "\n",
    "new_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate frames and slices directly from a .aedat4 file ?\n",
    "\n",
    "We have some convenience functions to directly load aedat4 files and preprocess into slices and corresponding frames.\n",
    "\n",
    "NOTE: This only works for aedat4 files where the shape of the data is inferred directly from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(486, 2, 240, 320)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aermanager.dataset_generator import dataset_content_generator\n",
    "\n",
    "sliced_events, frames, _ = dataset_content_generator(filename, time_window=3000)\n",
    "\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method can be used to slice data either by `time_window` or by `spike_count` by passing the corresponding named parameter to the method. You can also crop the data by passing a `crop_size` parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate a dataset from a list of files and labels ?\n",
    "\n",
    "For a typical machine learning task, we will need to use multiple data recordings and correspoindin file labels structured as a dataset. Such a dataset could then be used to train and test models.\n",
    "\n",
    "You can generate such a dataset using the method `gen_dataset_from_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from aermanager.dataset_generator import gen_dataset_from_list\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "event_files = [\n",
    "    \"test/data/class1/test.aedat4\", \n",
    "    \"test/data/class2/data_sample.aedat4\"\n",
    "]\n",
    "\n",
    "labels = [\"class1\", \"class2\"]\n",
    "\n",
    "\n",
    "gen_dataset_from_list(\n",
    "    event_files=event_files, \n",
    "    labels=labels, \n",
    "    destination_path=\"./example_dataset/\",\n",
    "    time_window=30000,\n",
    "    parser=parse_aedat4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a set of files and folders, each corresponding to one data recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./example_dataset/:\r\n",
      "\u001b[0m\u001b[01;34mdata_sample_bcac0\u001b[0m/  \u001b[01;34mtest_a98d9\u001b[0m/\r\n",
      "\r\n",
      "./example_dataset/data_sample_bcac0:\r\n",
      "0.h5   14.h5  19.h5  23.h5  28.h5  32.h5  37.h5  41.h5  46.h5  7.h5\r\n",
      "10.h5  15.h5  1.h5   24.h5  29.h5  33.h5  38.h5  42.h5  47.h5  8.h5\r\n",
      "11.h5  16.h5  20.h5  25.h5  2.h5   34.h5  39.h5  43.h5  4.h5   9.h5\r\n",
      "12.h5  17.h5  21.h5  26.h5  30.h5  35.h5  3.h5   44.h5  5.h5\r\n",
      "13.h5  18.h5  22.h5  27.h5  31.h5  36.h5  40.h5  45.h5  6.h5\r\n",
      "\r\n",
      "./example_dataset/test_a98d9:\r\n",
      "0.h5   11.h5  13.h5  15.h5  17.h5  1.h5  3.h5  5.h5  7.h5  9.h5\r\n",
      "10.h5  12.h5  14.h5  16.h5  18.h5  2.h5  4.h5  6.h5  8.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls -R \"./example_dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV files\n",
    "For convenience, the files and labels can also be passed via a csv file by using the method `gen_dataset_from_csv`.\n",
    "\n",
    "### Folders\n",
    "Alternatively, if your data is stored in structured folders, with each folder corresponding to a label, you can use the method `gen_dataset_from_folders`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load a dataset (for a dataloader)\n",
    "\n",
    "Once you have extracted and created a set of files for a dataset as described above, you can use our convenience dataloaders as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aermanager.datasets import SpikeTrainDataset, FramesDataset\n",
    "\n",
    "# A dataset with sliced spike trains\n",
    "st_dataset = SpikeTrainDataset(source_folder=\"./example_dataset/\")  \n",
    "\n",
    "# A dataset for pre-generated frames\n",
    "frames_dataset = FramesDataset(source_folder=\"./example_dataset/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over the data from the dataset objects like any regular iterator, or pass it to your favorite dataloader. In the below cell we print all the labels in the dataset we just used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'class2', b'class2', b'class2', b'class2', b'class2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[label for data, label in frames_dataset][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
