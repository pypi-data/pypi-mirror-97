@article{astropy1,
Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
Adsurl = {http://adsabs.harvard.edu/abs/2013A%26A...558A..33A},
Archiveprefix = {arXiv},
Author = {{Astropy Collaboration} and {Robitaille}, T.~P. and {Tollerud}, E.~J. and {Greenfield}, P. and {Droettboom}, M. and {Bray}, E. and {Aldcroft}, T. and {Davis}, M. and {Ginsburg}, A. and {Price-Whelan}, A.~M. and {Kerzendorf}, W.~E. and {Conley}, A. and {Crighton}, N. and {Barbary}, K. and {Muna}, D. and {Ferguson}, H. and {Grollier}, F. and {Parikh}, M.~M. and {Nair}, P.~H. and {Unther}, H.~M. and {Deil}, C. and {Woillez}, J. and {Conseil}, S. and {Kramer}, R. and {Turner}, J.~E.~H. and {Singer}, L. and {Fox}, R. and {Weaver}, B.~A. and {Zabalza}, V. and {Edwards}, Z.~I. and {Azalee Bostroem}, K. and {Burke}, D.~J. and {Casey}, A.~R. and {Crawford}, S.~M. and {Dencheva}, N. and {Ely}, J. and {Jenness}, T. and {Labrie}, K. and {Lim}, P.~L. and {Pierfederici}, F. and {Pontzen}, A. and {Ptak}, A. and {Refsdal}, B. and {Servillat}, M. and {Streicher}, O.},
Doi = {10.1051/0004-6361/201322068},
Eid = {A33},
Eprint = {1307.6212},
Journal = {\aap},
Keywords = {methods: data analysis, methods: miscellaneous, virtual observatory tools},
Month = oct,
Pages = {A33},
Primaryclass = {astro-ph.IM},
Title = {{Astropy: A community Python package for astronomy}},
Volume = 558,
Year = 2013,
Bdsk-Url-1 = {https://dx.doi.org/10.1051/0004-6361/201322068}}
@article{astropy2,
Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
Adsurl = {https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T},
Author = {{Price-Whelan}, A.~M. and {Sip{\H{o}}cz}, B.~M. and {G{\"u}nther}, H.~M. and {Lim}, P.~L. and {Crawford}, S.~M. and {Conseil}, S. and {Shupe}, D.~L. and {Craig}, M.~W. and {Dencheva}, N. and {Ginsburg}, A. and {VanderPlas}, J.~T. and {Bradley}, L.~D. and {P{\'e}rez-Su{\'a}rez}, D. and {de Val-Borro}, M. and {Paper Contributors}, (Primary and {Aldcroft}, T.~L. and {Cruz}, K.~L. and {Robitaille}, T.~P. and {Tollerud}, E.~J. and {Coordination Committee}, (Astropy and {Ardelean}, C. and {Babej}, T. and {Bach}, Y.~P. and {Bachetti}, M. and {Bakanov}, A.~V. and {Bamford}, S.~P. and {Barentsen}, G. and {Barmby}, P. and {Baumbach}, A. and {Berry}, K.~L. and {Biscani}, F. and {Boquien}, M. and {Bostroem}, K.~A. and {Bouma}, L.~G. and {Brammer}, G.~B. and {Bray}, E.~M. and {Breytenbach}, H. and {Buddelmeijer}, H. and {Burke}, D.~J. and {Calderone}, G. and {Cano Rodr{\'\i}guez}, J.~L. and {Cara}, M. and {Cardoso}, J.~V.~M. and {Cheedella}, S. and {Copin}, Y. and {Corrales}, L. and {Crichton}, D. and {D{\textquoteright}Avella}, D. and {Deil}, C. and {Depagne}, {\'E}. and {Dietrich}, J.~P. and {Donath}, A. and {Droettboom}, M. and {Earl}, N. and {Erben}, T. and {Fabbro}, S. and {Ferreira}, L.~A. and {Finethy}, T. and {Fox}, R.~T. and {Garrison}, L.~H. and {Gibbons}, S.~L.~J. and {Goldstein}, D.~A. and {Gommers}, R. and {Greco}, J.~P. and {Greenfield}, P. and {Groener}, A.~M. and {Grollier}, F. and {Hagen}, A. and {Hirst}, P. and {Homeier}, D. and {Horton}, A.~J. and {Hosseinzadeh}, G. and {Hu}, L. and {Hunkeler}, J.~S. and {Ivezi{\'c}}, {\v{Z}}. and {Jain}, A. and {Jenness}, T. and {Kanarek}, G. and {Kendrew}, S. and {Kern}, N.~S. and {Kerzendorf}, W.~E. and {Khvalko}, A. and {King}, J. and {Kirkby}, D. and {Kulkarni}, A.~M. and {Kumar}, A. and {Lee}, A. and {Lenz}, D. and {Littlefair}, S.~P. and {Ma}, Z. and {Macleod}, D.~M. and {Mastropietro}, M. and {McCully}, C. and {Montagnac}, S. and {Morris}, B.~M. and {Mueller}, M. and {Mumford}, S.~J. and {Muna}, D. and {Murphy}, N.~A. and {Nelson}, S. and {Nguyen}, G.~H. and {Ninan}, J.~P. and {N{\"o}the}, M. and {Ogaz}, S. and {Oh}, S. and {Parejko}, J.~K. and {Parley}, N. and {Pascual}, S. and {Patil}, R. and {Patil}, A.~A. and {Plunkett}, A.~L. and {Prochaska}, J.~X. and {Rastogi}, T. and {Reddy Janga}, V. and {Sabater}, J. and {Sakurikar}, P. and {Seifert}, M. and {Sherbert}, L.~E. and {Sherwood-Taylor}, H. and {Shih}, A.~Y. and {Sick}, J. and {Silbiger}, M.~T. and {Singanamalla}, S. and {Singer}, L.~P. and {Sladen}, P.~H. and {Sooley}, K.~A. and {Sornarajah}, S. and {Streicher}, O. and {Teuben}, P. and {Thomas}, S.~W. and {Tremblay}, G.~R. and {Turner}, J.~E.~H. and {Terr{\'o}n}, V. and {van Kerkwijk}, M.~H. and {de la Vega}, A. and {Watkins}, L.~L. and {Weaver}, B.~A. and {Whitmore}, J.~B. and {Woillez}, J. and {Zabalza}, V. and {Contributors}, (Astropy},
Doi = {10.3847/1538-3881/aabc4f},
Eid = {123},
Journal = {\aj},
Keywords = {methods: data analysis, methods: miscellaneous, methods: statistical, reference systems, Astrophysics - Instrumentation and Methods for Astrophysics},
Month = Sep,
Pages = {123},
Primaryclass = {astro-ph.IM},
Title = {{The Astropy Project: Building an Open-science Project and Status of the v2.0 Core Package}},
Volume = {156},
Year = 2018,
Bdsk-Url-1 = {https://doi.org/10.3847/1538-3881/aabc4f}}
@article{PyLops,
abstract = {Linear operators and optimisation are at the core of many algorithms used in signal and image processing, remote sensing, and inverse problems. For small to medium-scale problems, existing software packages (e.g., MATLAB, Python numpy and scipy) allow for explicitly building dense (or sparse) matrices and performing algebraic operations (e.g., computation of matrix-vector products and manipulation of matrices) with syntax that closely represents their corresponding analytical forms. However, many real application, large-scale operators do not lend themselves to explicit matrix representations, usually forcing practitioners to forego of the convenient linear-algebra syntax available for their explicit-matrix counterparts. PyLops is an open-source Python library providing a flexible and scalable framework for the creation and combination of so-called linear operators, class-based entities that represent matrices and inherit their associated syntax convenience, but do not rely on the creation of explicit matrices. We show that PyLops operators can dramatically reduce the memory load and CPU computations compared to explicit-matrix calculations, while still allowing users to seamlessly use their existing knowledge of compact matrix-based syntax that scales to any problem size because no explicit matrices are required.},
archivePrefix = {arXiv},
arxivId = {1907.12349},
author = {Ravasi, Matteo and Vasconcelos, Ivan},
eprint = {1907.12349},
file = {:home/jammy/Documents/Papers/Software/PyLops.pdf:pdf},
title = {{PyLops -- A Linear-Operator Python Library for large scale optimization}},
url = {http://arxiv.org/abs/1907.12349},
year = {2019}
}
@article{colossus,
abstract = {This paper introduces Colossus, a public, open-source python package for calculations related to cosmology, the large-scale structure (LSS) of matter in the universe, and the properties of dark matter halos. The code is designed to be fast and easy to use, with a coherent, well-documented user interface. The cosmology module implements Friedman-Lemaitre-Robertson-Walker cosmologies including curvature, relativistic species, and different dark energy equations of state, and provides fast computations of the linear matter power spectrum, variance, and correlation function. The LSS module is concerned with the properties of peaks in Gaussian random fields and halos in a statistical sense, including their peak height, peak curvature, halo bias, and mass function. The halo module deals with spherical overdensity radii and masses, density profiles, concentration, and the splashback radius. To facilitate the rapid exploration of these quantities, Colossus implements more than 40 different fitting functions from the literature. I discuss the core routines in detail, with particular emphasis on their accuracy. Colossus is available at bitbucket.org/bdiemer/colossus.},
archivePrefix = {arXiv},
arxivId = {1712.04512},
author = {Diemer, Benedikt},
doi = {10.3847/1538-4365/aaee8c},
eprint = {1712.04512},
file = {:home/jammy/Documents/Papers/Software/Collosus2018.pdf:pdf},
issn = {0067-0049},
journal = {The Astrophysical Journal Supplement Series},
keywords = {cosmology,cosmology: theory,methods: numerical,methods,numerical,theory},
number = {2},
pages = {35},
publisher = {IOP Publishing},
title = {{COLOSSUS: A Python Toolkit for Cosmology, Large-scale Structure, and Dark Matter Halos}},
url = {http://dx.doi.org/10.3847/1538-4365/aaee8c},
volume = {239},
year = {2018}
}
@article{corner,
  doi = {10.21105/joss.00024},
  url = {https://doi.org/10.21105/joss.00024},
  year  = {2016},
  month = {jun},
  publisher = {The Open Journal},
  volume = {1},
  number = {2},
  pages = {24},
  author = {Daniel Foreman-Mackey},
  title = {corner.py: Scatterplot matrices in Python},
  journal = {The Journal of Open Source Software}
}
@article{dynesty,
abstract = {We present dynesty, a public, open-source, python package to estimate Bayesian posteriors and evidences (marginal likelihoods) using the dynamic nested sampling methods developed by Higson etÂ al. By adaptively allocating samples based on posterior structure, dynamic nested sampling has the benefits of Markov chain Monte Carlo (MCMC) algorithms that focus exclusively on posterior estimation while retaining nested sampling's ability to estimate evidences and sample from complex, multimodal distributions. We provide an overview of nested sampling, its extension to dynamic nested sampling, the algorithmic challenges involved, and the various approaches taken to solve them in this and previous work. We then examine dynesty's performance on a variety of toy problems along with several astronomical applications. We find in particular problems dynesty can provide substantial improvements in sampling efficiency compared to popular MCMC approaches in the astronomical literature. More detailed statistical results related to nested sampling are also included in the appendix.},
archivePrefix = {arXiv},
arxivId = {1904.02180},
author = {Speagle, Joshua S},
doi = {10.1093/mnras/staa278},
eprint = {1904.02180},
file = {:home/jammy/Documents/Papers/PPLs/Dynesty.pdf:pdf},
issn = {0035-8711},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {data analysis,methods,statistical},
number = {3},
pages = {3132--3158},
title = {{dynesty: a dynamic nested sampling package for estimating Bayesian posteriors and evidences}},
volume = {493},
year = {2020}
}
@article{emcee,
abstract = {We introduce a stable, well tested Python implementation of the affine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed by Goodman {\&} Weare (2010). The code is open source and has already been used in several published projects in the astrophysics literature. The algorithm behind emcee has several advantages over traditional MCMC sampling methods and it has excellent performance as measured by the autocorrelation time (or function calls per independent sample). One major advantage of the algorithm is that it requires hand-tuning of only 1 or 2 parameters compared to {\$}\backslashsim N{\^{}}2{\$} for a traditional algorithm in an N-dimensional parameter space. In this document, we describe the algorithm and the details of our implementation and API. Exploiting the parallelism of the ensemble method, emcee permits any user to take advantage of multiple CPU cores without extra effort. The code is available online at http://dan.iel.fm/emcee under the MIT License.},
archivePrefix = {arXiv},
arxivId = {1202.3665},
author = {Foreman-Mackey, Daniel and Hogg, David W. and Lang, Dustin and Goodman, Jonathan},
doi = {10.1086/670067},
eprint = {1202.3665},
file = {:home/jammy/Documents/Papers/PPLs/Emcee.pdf:pdf},
issn = {00046280},
journal = {Publications of the Astronomical Society of the Pacific},
number = {925},
pages = {306--312},
title = {{emcee : The MCMC Hammer }},
volume = {125},
year = {2013}
}
@article{matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}
@article{numba,
abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is of-ten a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addi-tion, we share our experience in building a JIT compiler using LLVM[1].},
author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
doi = {10.1145/2833157.2833162},
file = {:home/jammy/Documents/Papers/Software/numba{\_}sc15.pdf:pdf},
isbn = {9781450340052},
journal = {Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC - LLVM '15},
keywords = {2,a jit for numeric,com-,compiler,jit,just-in-time,llvm,numba is a function-at-a-time,python},
pages = {1--6},
title = {{Numba: a LLVM-based Python JIT compiler}},
url = {http://dl.acm.org/citation.cfm?doid=2833157.2833162},
year = {2015}
}
@article{numpy,
  author={S. {van der Walt} and S. C. {Colbert} and G. {Varoquaux}},
  doi={10.1109/MCSE.2011.37},
  journal={Computing in Science Engineering},
  title={The NumPy Array2D: A Structure for Efficient Numerical Computation},
  year={2011},
  volume={13},
  number={2},
  pages={22-30},}
@article{pyautofit,
  doi = {10.21105/joss.02550},
  url = {https://doi.org/10.21105/joss.02550},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {58},
  pages = {2550},
  author = {Nightingale, J. W. and Hayes, R. G. and Griffiths, M.},
  title = {`PyAutoFit`: A Classy Probabilistic Programming Language for Model Composition and Fitting},
  journal = {Journal of Open Source Software}
}
@article{pyautolens,
  doi = {10.21105/joss.02825},
  url = {https://doi.org/10.21105/joss.02825},
  year = {2021},
  publisher = {The Open Journal},
  volume = {6},
  number = {58},
  pages = {2825},
  author = {Nightingale, J. W. and Hayes, R. G. and Ashley Kelly and Aristeidis Amvrosiadis and Amy Etherington and Qiuhan He and Nan Li and XiaoYue Cao and Jonathan Frawley and Shaun Cole and Andrea Enia and Carlos S. Frenk and David R. Harvey and Ran Li and Richard J. Massey and Mattia Negrello and Andrew Robertson},
  title = {`PyAutoLens`: Open-Source Strong Gravitational Lensing},
  journal = {Journal of Open Source Software}
}
@article{multinest,
abstract = {We present further development and the first public release of our multimodal nested sampling algorithm, called MultiNest. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz and Hobson, which itself significantly outperformed existing Markov chain Monte Carlo techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MultiNest algorithm are demonstrated by application to two toy problems and to a cosmological inference problem focusing on the extension of the vanilla $\Lambda$ cold dark matter model to include spatial curvature and a varying equation of state for dark energy. The MultiNest software, which is fully parallelized using MPI and includes an interface to CosmoMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SuperBayeS package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org. {\textcopyright} 2009 RAS.},
archivePrefix = {arXiv},
arxivId = {0809.3437},
author = {Feroz, F. and Hobson, M. P. and Bridges, M.},
doi = {10.1111/j.1365-2966.2009.14548.x},
eprint = {0809.3437},
isbn = {0035-8711},
issn = {00358711},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Methods: Data analysis,Methods: Statistical},
number = {4},
pages = {1601--1614},
pmid = {29176},
title = {{MultiNest: An efficient and robust Bayesian inference tool for cosmology and particle physics}},
volume = {398},
year = {2009}
}
@article{pymultinest,
abstract = {Context. Aims. Active galactic nuclei are known to have complex X-ray spectra that depend on both the properties of the accreting super-massive black hole (e.g. mass, accretion rate) and the distribution of obscuring material in its vicinity (i.e. the "torus"). Often however, simple and even unphysical models are adopted to represent the X-ray spectra of AGN, which do not capture the complexity and diversity of the observations. In the case of blank field surveys in particular, this should have an impact on e.g. the determination of the AGN luminosity function, the inferred accretion history of the Universe and also on our understanding of the relation between AGN and their host galaxies. Methods. We develop a Bayesian framework for model comparison and parameter estimation of X-ray spectra. We take into account uncertainties associated with both the Poisson nature of X-ray data and the determination of source redshift using photometric methods. We also demonstrate how Bayesian model comparison can be used to select among ten different physically motivated X-ray spectral models the one that provides a better representation of the observations. This methodology is applied to X-ray AGN in the 4 Ms Chandra Deep Field South. Results. For the {\~{}}350 AGN in that field, our analysis identifies four components needed to represent the diversity of the observed X-ray spectra: (1) an intrinsic power law; (2) a cold obscurer which reprocesses the radiation due to photo-electric absorption, Compton scattering and Fe-K fluorescence; (3) an unabsorbed power law associated with Thomson scattering off ionised clouds; and (4) Compton reflection, most noticeable from a stronger-than-expected Fe-K line. Simpler models, such as a photo-electrically absorbed power law with a Thomson scattering component, are ruled out with decisive evidence (B {\textgreater} 100). We also find that ignoring the Thomson scattering component results in underestimation of the inferred column density, NH, of the obscurer. Regarding the geometry of the obscurer, there is strong evidence against both a completely closed (e.g. sphere), or entirely open (e.g. blob of material along the line of sight), toroidal geometry in favour of an intermediate case. Conclusions. Despite the use of low-count spectra, our methodology is able to draw strong inferences on the geometry of the torus. Simpler models are ruled out in favour of a geometrically extended structure with significant Compton scattering. We confirm the presence of a soft component, possibly associated with Thomson scattering off ionised clouds in the opening angle of the torus. The additional Compton reflection required by data over that predicted by toroidal geometry models, may be a sign of a density gradient in the torus or reflection off the accretion disk. Finally, we release a catalogue of AGN in the CDFS with estimated parameters such as the accretion luminosity in the 2-10 keV band and the column density, NH, of the obscurer. {\textcopyright} ESO, 2014.},
archivePrefix = {arXiv},
arxivId = {1402.0004},
author = {Buchner, J. and Georgakakis, A. and Nandra, K. and Hsu, L. and Rangel, C. and Brightman, M. and Merloni, A. and Salvato, M. and Donley, J. and Kocevski, D.},
doi = {10.1051/0004-6361/201322971},
eprint = {1402.0004},
file = {:home/jammy/Documents/Papers/Stats/ButchnerPyMultiNest.pdf:pdf},
issn = {14320746},
journal = {Astronomy and Astrophysics},
keywords = {Accretion, accretion disks,Galaxies: high-redshift,Galaxies: nuclei,Methods: data analysis,Methods: statistical,X-rays: galaxies},
pages = {A125},
title = {{X-ray spectral modelling of the AGN obscuring region in the CDFS: Bayesian model selection and catalogue}},
volume = {564},
year = {2014}
}
@article{pynufft,
abstract = {A Python non-uniform fast Fourier transform (PyNUFFT) package has been developed to accelerate multidimensional non-Cartesian image reconstruction on heterogeneous platforms. Since scientific computing with Python encompasses a mature and integrated environment, the time efficiency of the NUFFT algorithm has been a major obstacle to real-time non-Cartesian image reconstruction with Python. The current PyNUFFT software enables multi-dimensional NUFFT accelerated on a heterogeneous platform, which yields an efficient solution to many non-Cartesian imaging problems. The PyNUFFT also provides several solvers, including the conjugate gradient method, 1 total variation regularized ordinary least square (L1TV-OLS), and 1 total variation regularized least absolute deviation (L1TV-LAD). Metaprogramming libraries have been employed to accelerate PyNUFFT. The PyNUFFT package has been tested on multi-core central processing units (CPUs) and graphic processing units (GPUs), with acceleration factors of 6.3â9.5Ã on a 32-thread CPU platform and 5.4â13Ã on a GPU.},
author = {Lin, Jyh Miin},
doi = {10.3390/jimaging4030051},
file = {:home/jammy/Documents/Papers/Software/jimaging-04-00051-v2.pdf:pdf},
issn = {2313433X},
journal = {Journal of Imaging},
keywords = {Graphic processing unit (GPU),Heterogeneous system architecture (HSA),Magnetic resonance imaging (MRI),Multi-core system,Total variation (TV)},
number = {3},
pages = {1--22},
title = {{Python non-uniform fast fourier transform (PyNUFFT): An accelerated non-cartesian MRI package on a heterogeneous platform (CPU/GPU)}},
volume = {4},
year = {2018}
}
@software{pyquad,
  author       = {Ashley J. Kelly},
  title        = {pyquad},
  month        = jul,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {0.6.4},
  doi          = {10.5281/zenodo.3936959},
  url          = {https://doi.org/10.5281/zenodo.3936959}
}
@article{pyswarms,
    author  = {Lester James V. Miranda},
    title   = "{P}y{S}warms, a research-toolkit for {P}article {S}warm {O}ptimization in {P}ython",
    journal = {Journal of Open Source Software},
    year    = {2018},
    volume  = {3},
    issue   = {21},
    doi     = {10.21105/joss.00433},
    url     = {https://doi.org/10.21105/joss.00433}
}
 @book{python,
 author = {Van Rossum, Guido and Drake, Fred L.},
 title = {Python 3 Reference Manual},
 year = {2009},
 isbn = {1441412697},
 publisher = {CreateSpace},
 address = {Scotts Valley, CA}
}
@article{scikit-image,
  title={scikit-image: image processing in Python},
  author={Van der Walt, Stefan and Sch{\"o}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  journal={PeerJ},
  volume={2},
  pages={e453},
  year={2014},
  publisher={PeerJ Inc.}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@article{scipy,
       author = {{Virtanen}, Pauli and {Gommers}, Ralf and {Oliphant},
         Travis E. and {Haberland}, Matt and {Reddy}, Tyler and
         {Cournapeau}, David and {Burovski}, Evgeni and {Peterson}, Pearu
         and {Weckesser}, Warren and {Bright}, Jonathan and {van der Walt},
         St{\'e}fan J.  and {Brett}, Matthew and {Wilson}, Joshua and
         {Jarrod Millman}, K.  and {Mayorov}, Nikolay and {Nelson}, Andrew
         R.~J. and {Jones}, Eric and {Kern}, Robert and {Larson}, Eric and
         {Carey}, CJ and {Polat}, {\.I}lhan and {Feng}, Yu and {Moore},
         Eric W. and {Vand erPlas}, Jake and {Laxalde}, Denis and
         {Perktold}, Josef and {Cimrman}, Robert and {Henriksen}, Ian and
         {Quintero}, E.~A. and {Harris}, Charles R and {Archibald}, Anne M.
         and {Ribeiro}, Ant{\^o}nio H. and {Pedregosa}, Fabian and
         {van Mulbregt}, Paul and {Contributors}, SciPy 1. 0},
        title = "{SciPy 1.0: Fundamental Algorithms for Scientific
                  Computing in Python}",
      journal = {Nature Methods},
      year = "2020",
      volume={17},
      pages={261--272},
      adsurl = {https://rdcu.be/b08Wh},
      doi = {10.1038/s41592-019-0686-2},
}
@article{Nightingale2015,
abstract = {We present a new pixelized method for the inversion of gravitationally lensed extended source images which we term adaptive semi-linear inversion (SLI). At the heart of the method is an h-means clustering algorithm which is used to derive a source plane pixelization that adapts to the lens model magnification. The distinguishing feature of adaptive SLI is that every pixelization is derived from a random initialization, ensuring that data discretization is performed in a completely different and unique way for every lens model parameter set. We compare standard SLI on a fixed source pixel grid with the new method and demonstrate the shortcomings of the former when modelling singular power-law ellipsoid (SPLE) lens profiles. In particular, we demonstrate the superior reliability and efficiency of adaptive SLI which, by design, fixes the number of degrees of freedom (NDOF) of the optimization and thereby removes biases present with other methods that allow the NDOF to vary. In addition, we highlight the importance of data discretization in pixel-based inversion methods, showing that adaptive SLI averages over significant systematics that are present when a fixed source pixel grid is used. In the case of the SPLE lens profile, we show how the method successfully samples its highly degenerate posterior probability distribution function with a single nonlinear search. The robustness of adaptive SLI provides a firm foundation for the development of a strong lens modelling pipeline, which will become necessary in the short-term future to cope with the increasing rate of discovery of new strong lens systems.},
archivePrefix = {arXiv},
arxivId = {1412.7436},
author = {Nightingale, J. W. and Dye, S.},
doi = {10.1093/mnras/stv1455},
eprint = {1412.7436},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Galaxies: evolution,Galaxies: structure,Methods: observational},
month = {sep},
number = {3},
pages = {2940--2959},
title = {{Adaptive semi-linear inversion of strong gravitational lens imaging}},
volume = {452},
year = {2015}
}
@article{Nightingale2018,
abstract = {This work presents AutoLens, the first entirely automated modeling suite for the analysis of galaxy-scale strong gravitational lenses. AutoLens simultaneously models the lens galaxy's light and mass whilst reconstructing the extended source galaxy on an adaptive pixel-grid. The method's approach to source-plane discretization is amorphous, adapting its clustering and regularization to the intrinsic properties of the lensed source. The lens's light is fitted using a superposition of Sersic functions, allowing AutoLens to cleanly deblend its light from the source. Single-component mass models representing the lens's total mass density profile are demonstrated, which in conjunction with light modeling can detect central images using a centrally cored profile. Decomposed mass modeling is also shown, which can fully decouple a lens's light and dark matter and determine whether the two components are geometrically aligned. The complexity of the light and mass models is automatically chosen via Bayesian model comparison. These steps form AutoLens's automated analysis pipeline, such that all results in this work are generated without any user intervention. This is rigorously tested on a large suite of simulated images, assessing its performance on a broad range of lens profiles, source morphologies, and lensing geometries. The method's performance is excellent, with accurate light, mass, and source profiles inferred for data sets representative of both existing Hubble imaging and future Euclid wide-field observations.},
archivePrefix = {arXiv},
arxivId = {1708.07377},
author = {Nightingale, J. W. and Dye, S. and Massey, Richard J.},
doi = {10.1093/mnras/sty1264},
eprint = {1708.07377},
file = {:home/jammy/Documents/Papers{\_}Me/AutoLensChangesMarked.pdf:pdf},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Galaxy: structure,Gravitational lensing,Methods: data analysis},
number = {4},
pages = {4738--4784},
title = {{AutoLens: Automated modeling of a strong lens's light, mass, and source}},
url = {https://academic.oup.com/mnras/article/478/4/4738/5001434},
volume = {478},
year = {2018}
}
@article{Nightingale2019,
abstract = {We investigate how strong gravitational lensing can test contemporary models of massive elliptical (ME) galaxy formation, by combining a traditional decomposition of their visible stellar distribution with a lensing analysis of their mass distribution. As a proof of concept, we study a sample of three ME lenses, observing that all are composed of two distinct baryonic structures, a 'red' central bulge surrounded by an extended envelope of stellar material. Whilst these two components look photometrically similar, their distinct lensing effects permit a clean decomposition of their mass structure. This allows us to infer two key pieces of information about each lens galaxy: (i) the stellar mass distribution (without invoking stellar populations models) and (ii) the inner dark matter halo mass. We argue that these two measurements are crucial to testing models of ME formation, as the stellar mass profile provides a diagnostic of baryonic accretion and feedback whilst the dark matter mass places each galaxy in the context of LCDM large-scale structure formation. We also detect large rotational offsets between the two stellar components and a lopsidedness in their outer mass distributions, which hold further information on the evolution of each ME. Finally, we discuss how this approach can be extended to galaxies of all Hubble types and what implication our results have for studies of strong gravitational lensing.},
archivePrefix = {arXiv},
arxivId = {1901.07801},
author = {Nightingale, J. W. and Massey, Richard J. and Harvey, David R. and Cooper, Andrew P. and Etherington, Amy and Tam, Sut Ieng and Hayes, Richard G.},
doi = {10.1093/mnras/stz2220},
eprint = {1901.07801},
file = {:home/jammy/Documents/Papers{\_}Me/Gal{\_}Structure{\_}Final/GalaxyStructure.pdf:pdf},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Galaxies: Evolution,Galaxies: Formation,Gravitational lensing: Strong},
number = {2},
pages = {2049--2068},
title = {{Galaxy structure with strong gravitational lensing: Decomposing the internal mass distribution of massive elliptical galaxies}},
url = {http://arxiv.org/abs/1901.07801},
volume = {489},
year = {2019}
}
@article{Anowar2019,
    author = {Shajib, Anowar J},
    title = "{Unified lensing and kinematic analysis for any elliptical mass profile}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {488},
    number = {1},
    pages = {1387-1400},
    year = {2019},
    month = {07},
    abstract = "{We demonstrate an efficient method to compute the strong-gravitational-lensing deflection angle and magnification for any elliptical surface density profile. This method solves a numerical hurdle in lens modelling that has lacked a general solution for nearly three decades. The hurdle emerges because it is prohibitive to derive analytic expressions of the lensing quantities for most elliptical mass profiles. In our method, we first decompose an elliptical mass profile into concentric Gaussian components. We introduce an integral transform that provides us with a fast and accurate algorithm for this Gaussian decomposition. We derive analytic expressions of the lensing quantities for a Gaussian component. As a result, we can compute these quantities for the total mass profile by adding up the contributions from the individual components. This lensing analysis self-consistently completes the kinematic description in terms of Gaussian components presented by Cappellari (2008). Our method is general without extra computational burden unlike other methods currently in use.}",
    issn = {0035-8711},
    doi = {10.1093/mnras/stz1796},
    url = {https://doi.org/10.1093/mnras/stz1796},
    eprint = {https://academic.oup.com/mnras/article-pdf/488/1/1387/28937972/stz1796.pdf},
}