{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Anatomy of a Quantizer\n",
    "\n",
    "## What's in a Quantizer? \n",
    "\n",
    "In a broad sense, a quantizer is anything that implements a quantization technique, and the flexibility of Brevitas means that there are different ways to do so.  \n",
    "However, to keep our terminology straight, we refer to a quantizer as a specific kind of way to implement quantization, the one preferred and adopted by default. That is, a quantizer is a subclass of a `brevitas.inject.ExtendedInjector` that carries a `tensor_quant` attribute, which points to an instance of a torch `Module` that implements quantization.\n",
    "\n",
    "We have seen in previous tutorials quantizers being imported from `brevitas.quant` and passed on to quantized layers. We can easily very what we just said on one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.inject import ExtendedInjector\n",
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat \n",
    "\n",
    "issubclass(Int8ActPerTensorFloat, ExtendedInjector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RescalingIntQuant(\n",
       "  (int_quant): IntQuant(\n",
       "    (float_to_int_impl): RoundSte()\n",
       "    (tensor_clamp_impl): TensorClamp()\n",
       "    (delay_wrapper): DelayWrapper(\n",
       "      (delay_impl): _NoDelay()\n",
       "    )\n",
       "  )\n",
       "  (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "    (stats_input_view_shape_impl): OverTensorView()\n",
       "    (stats): _Stats(\n",
       "      (stats_impl): AbsPercentile()\n",
       "    )\n",
       "    (restrict_clamp_scaling): _RestrictClampValue(\n",
       "      (clamp_min_ste): Identity()\n",
       "      (restrict_value_impl): FloatRestrictValue()\n",
       "    )\n",
       "    (restrict_inplace_preprocess): Identity()\n",
       "  )\n",
       "  (int_scaling_impl): IntScaling()\n",
       "  (zero_point_impl): ZeroZeroPoint(\n",
       "    (zero_point): StatelessBuffer()\n",
       "  )\n",
       "  (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "    (bit_width): StatelessBuffer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Int8ActPerTensorFloat.tensor_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we said *subclass* and not *instance*. To understand why that's the case, we have to understand what an `ExtendedInjector` is and why it's used in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with auto-wiring Dependency Injection\n",
    "\n",
    "Pytorch has exploded in popularity thanks to its straightforward numpy-like *define-by-run* execution model. However, when it comes to applying quantization, this style of programming poses a problem.  \n",
    "\n",
    "Many quantization methods depend on making decisions based on the (in Pytorch terms) `state_dict` of the original floating-point model to finetune with quantization. However, when we instantiate a model in Pytorch we can't know on the spot if a state_dict is going to be loaded a few lines of code later or not. Yet, because Pytorch is define-by-run, we need our model to work consistently both before and after a `state_dict` is possibly loaded. In a traditional scenario that wouldn't pose a problem. However, with quantization in the loop, the way a quantizer is defined might change before and after a pretrained `state_dict` is loaded.\n",
    "\n",
    "That means that we need a way to define our quantized model such that it can react appropriately in case the `state_dict` changes. In a Python-only world that wouldn't be too hard. However, in order to mitigate the performance impact of quantization-aware training, Brevitas makes extended use of Pytorch's JIT compiler for a custom subset of Python, TorchScript. That means that in most scenarios, when a `state_dict` is loaded, we need to recompile parts of the model. Because compilation in general is a lossy process, a TorchScript component cannot simply re-compile itself based on new input information. \n",
    "\n",
    "We need then a way to *declare* a quantization method such that it can be re-initialized and JIT compiled any time the `state_dict` changes. Because we want to support arbitrarly-complex user-defined quantization algorithms, this method has to be generic, i.e. it cannot depend on the specifics of the quantization algorithm implemented.\n",
    "\n",
    "Implementing a quantizer with an `ExtendedInjector` is a way to do so. Specifically, an `ExtendedInjector` extends an `Injector` from an older version (*0.2.1*) of the excellent dependency-injection library *[dependencies](https://github.com/proofit404/dependencies)* with support for a couple of extra features that are specific to Brevitas' needs.\n",
    "\n",
    "An `Injector` (and an `ExtendedInjector`) allows to take what might be a very complicated graph of interwined objects and turns it into a flat list of variables that are capable of auto-assembly by matching variable names to arguments names. This technique typically goes under the name of auto-wiring dependency injection. \n",
    "\n",
    "In the context of Brevitas, the goal is gather all the modules and hyperparameters that contribute to a quantization implementation such that they can be re-assembled automatically on demand. What comes out of this process is a `tensor_quant` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Practical Example: Binary Quantization\n",
    "\n",
    "To make things practical, let's look at how we can implement a simple variant of binary quantization. All the components typically used to implement quantization can be found under `brevitas.core`. As mentioned before, Brevitas makes heavy use of TorchScript. In particular, all the components found under `brevitas.core` are implemented as `ScriptModule` that can be assembled together.\n",
    "The core `ScriptModule` that implements binarization can be found under `brevitas.core.quant`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def pretty_print_source(source):\n",
    "    display(Markdown('```python\\n' + source + '\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class BinaryQuant(brevitas.jit.ScriptModule):\n",
       "    \"\"\"\n",
       "    ScriptModule that implements scaled uniform binary quantization of an input tensor.\n",
       "    Quantization is performed with :func:`~brevitas.function.ops_ste.binary_sign_ste`.\n",
       "\n",
       "    Args:\n",
       "        scaling_impl (Module): Module that returns a scale factor.\n",
       "        quant_delay_steps (int): Number of training steps to delay quantization for. Default: 0\n",
       "\n",
       "    Returns:\n",
       "        Tuple[Tensor, Tensor, Tensor, Tensor]: Quantized output in de-quantized format, scale,\n",
       "            zero-point, bit_width.\n",
       "\n",
       "    Examples:\n",
       "        >>> from brevitas.core.scaling import ConstScaling\n",
       "        >>> binary_quant = BinaryQuant(ConstScaling(0.1))\n",
       "        >>> inp = torch.Tensor([0.04, -0.6, 3.3])\n",
       "        >>> out, scale, zero_point, bit_width = binary_quant(inp)\n",
       "        >>> out\n",
       "        tensor([ 0.1000, -0.1000,  0.1000])\n",
       "        >>> scale\n",
       "        tensor(0.1000)\n",
       "        >>> zero_point\n",
       "        tensor(0.)\n",
       "        >>> bit_width\n",
       "        tensor(1.)\n",
       "\n",
       "    Note:\n",
       "        Maps to quant_type == QuantType.BINARY == 'BINARY' == 'binary' when applied to weights\n",
       "         in higher-level APIs.\n",
       "\n",
       "    Note:\n",
       "        Set env variable BREVITAS_JIT=1 to enable TorchScript compilation of this module.\n",
       "    \"\"\"\n",
       "\n",
       "    def __init__(self, scaling_impl: Module, quant_delay_steps: int = 0):\n",
       "        super(BinaryQuant, self).__init__()\n",
       "        self.scaling_impl = scaling_impl\n",
       "        self.bit_width = BitWidthConst(1)\n",
       "        self.zero_point = StatelessBuffer(torch.tensor(0.0))\n",
       "        self.delay_wrapper = DelayWrapper(quant_delay_steps)\n",
       "\n",
       "    @brevitas.jit.script_method\n",
       "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
       "        scale = self.scaling_impl(x)\n",
       "        y = binary_sign_ste(x) * scale\n",
       "        y = self.delay_wrapper(x, y)\n",
       "        return y, scale, self.zero_point(), self.bit_width()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from brevitas.core.quant import BinaryQuant\n",
    "\n",
    "source = inspect.getsource(BinaryQuant)  \n",
    "pretty_print_source(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation is quite simple. Apart from `quant_delay_steps`, which allows to delay quantization by a certain number of training steps (*default = 0*), the only other argument that BinaryQuant accepts is an implementation to compute the scale factor. `bit_width` is fixed to 1 and `zero-point` is fixed to 0.\n",
    "\n",
    "We pick as scale factor implementation a `ScriptModule` called `ParameterScaling`, which implements a learned parameter with user-defined initialization. It can be found under `brevitas.core.scaling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.scaling import ParameterScaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Binary Quantization\n",
    "\n",
    "As a first step, we simply instantiate `BinaryQuant` with `ParameterScaling` using `scaling_init` equal *0.1* and we call it on a random floating-point input tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1000,  0.1000,  0.1000,  0.1000],\n",
       "         [-0.1000, -0.1000,  0.1000,  0.1000],\n",
       "         [-0.1000,  0.1000,  0.1000,  0.1000],\n",
       "         [-0.1000, -0.1000,  0.1000,  0.1000]], grad_fn=<MulBackward0>),\n",
       " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "manual_tensor_quant = BinaryQuant(scaling_impl=ParameterScaling(scaling_init=0.1))\n",
    "manual_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too surprising here, as expected the tensor is binarized with the scale factor we defined. Note however how `manual_tensor_quant` is returning a `tuple` and not a `QuantTensor`. This is because support for custom data structures in TorchScript is still quite limited, so `QuantTensor` are allocated only in Python-world abstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Quantization with an ExtendedInjector\n",
    "\n",
    "Let's now declare `tensor_quant` through an `ExtendedInjector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1000,  0.1000, -0.1000,  0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000, -0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000, -0.1000],\n",
       "         [-0.1000, -0.1000,  0.1000, -0.1000]], grad_fn=<MulBackward0>),\n",
       " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.inject import ExtendedInjector\n",
    "\n",
    "class MyBinaryQuantizer(ExtendedInjector):\n",
    "    tensor_quant = BinaryQuant\n",
    "    scaling_impl=ParameterScaling\n",
    "    scaling_init=0.1\n",
    "\n",
    "inj_tensor_quant = MyBinaryQuantizer.tensor_quant\n",
    "inj_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time `MyBinaryQuantizer.tensor_quant` is called, a new instance of `BinaryQuant` is created. Note how the attributes of `MyBinaryQuantizer` are designed to match the name of the arguments of each other, except for `tensor_quant`, which is what we are interested in retrieving from the outside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inheritance and Composition of Quantizers\n",
    "\n",
    "The advantage of expressing a quantizer through a Python class also means that we can leverage both *inheritance* and *composition*. So for example we can inherit from `MyBinaryQuantizer` and override `scaling_init` with a new value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1., -1., -1., -1.],\n",
       "         [-1.,  1.,  1., -1.],\n",
       "         [ 1., -1.,  1., -1.],\n",
       "         [-1., -1., -1.,  1.]], grad_fn=<MulBackward0>),\n",
       " tensor(1., grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyChildBinaryQuantizer(MyBinaryQuantizer):\n",
    "    scaling_init=1.0\n",
    "    \n",
    "child_inj_tensor_quant = MyChildBinaryQuantizer.tensor_quant\n",
    "child_inj_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can leverage composition by assembling together various classes containing different pieces of a quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1000,  0.1000, -0.1000,  0.1000],\n",
       "         [-0.1000,  0.1000,  0.1000, -0.1000],\n",
       "         [ 0.1000,  0.1000, -0.1000,  0.1000],\n",
       "         [ 0.1000, -0.1000, -0.1000, -0.1000]], grad_fn=<MulBackward0>),\n",
       " tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>),\n",
       " tensor(0.),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyBinaryImpl(ExtendedInjector):\n",
    "    tensor_quant = BinaryQuant\n",
    "\n",
    "class MyScalingImpl(ExtendedInjector):\n",
    "    scaling_impl=ParameterScaling\n",
    "    scaling_init=0.1\n",
    "    \n",
    "class MyComposedBinaryQuantizer(MyBinaryImpl, MyScalingImpl):\n",
    "    pass\n",
    "\n",
    "comp_inj_tensor_quant = MyComposedBinaryQuantizer.tensor_quant\n",
    "comp_inj_tensor_quant(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing a Custom Quantizer to QuantConv2d\n",
    "\n",
    "As expected, we can pass the quantizer to a quantized layer such as QuantConv2d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1000,  0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [ 0.1000,  0.1000,  0.1000]],\n",
       "\n",
       "         [[ 0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000]],\n",
       "\n",
       "         [[ 0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=None, training=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantConv2d\n",
    "\n",
    "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryQuantizer)\n",
    "quant_weight = binary_weight_quant_conv.quant_weight()\n",
    "quant_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note however how the `QuantTensor` is not properly formed, as the `signed` attribute is `None`. This means that `quant_weight` is not considered valid, as the affine quantization invariant cannot be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_weight.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`signed` is one of those attributes that in the case of binary quantization has to be explicitly defined by the user. We can do so by simply setting it in the quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000, -0.1000]],\n",
       "\n",
       "         [[ 0.1000, -0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[-0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000],\n",
       "          [-0.1000, -0.1000,  0.1000]]],\n",
       "\n",
       "\n",
       "        [[[-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000]],\n",
       "\n",
       "         [[-0.1000, -0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000, -0.1000],\n",
       "          [-0.1000, -0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySignedBinaryQuantizer(MyBinaryQuantizer):\n",
    "    signed = True\n",
    "    \n",
    "binary_weight_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MySignedBinaryQuantizer)\n",
    "signed_quant_weight = binary_weight_quant_conv.quant_weight()\n",
    "signed_quant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_quant_weight.is_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the quant weights are valid.\n",
    "\n",
    "When we want to add or override an single attribute of a quantizer passed to a layer, defining a whole new quantizer can be too verbose. There is a simpler syntax to achieve the same goal. Let's say we want to have add the `signed` attribute to `MyBinaryQuantizer`, as we just did. We could have also simply done the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[ 0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[-0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[-0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000],\n",
       "          [ 0.1000, -0.1000,  0.1000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000, -0.1000]],\n",
       "\n",
       "         [[ 0.1000, -0.1000,  0.1000],\n",
       "          [-0.1000,  0.1000,  0.1000],\n",
       "          [ 0.1000, -0.1000, -0.1000]],\n",
       "\n",
       "         [[ 0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000, -0.1000, -0.1000],\n",
       "          [-0.1000, -0.1000,  0.1000]]]], grad_fn=<MulBackward0>), scale=tensor(0.1000, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_scale_quant_conv = QuantConv2d(3, 2, (3,3), weight_quant=MyBinaryQuantizer, weight_signed=True)\n",
    "small_scale_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did was to take the name of the attribute `scaling_init`, add the prefix `weight_`, and pass it as a keyword argument to `QuantConv2d`. What happens in the background is that the keyword arguments prefixed with `weight_` are set as attributes of `weight_quant`, possibly overriding any pre-existing value. The same principle applies to `input_`, `output_` and `bias_`.\n",
    "In case the corrisponding quantizer is `None`, a new *empty* `ExtendedInjector` is internally allocated and keyword arguments are passed as attributes to that.\n",
    "\n",
    "So for example we can define `output_quant` (which by default is None) completely *inline* without defining an explicit standalone quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1., -1., -1.],\n",
       "          [-1.,  1., -1.],\n",
       "          [-1.,  1., -1.]],\n",
       "\n",
       "         [[ 1.,  1., -1.],\n",
       "          [-1.,  1., -1.],\n",
       "          [-1.,  1.,  1.]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inline_output_quant = QuantConv2d(\n",
    "    3, 2, (3,3), \n",
    "    weight_quant=MySignedBinaryQuantizer,\n",
    "    output_tensor_quant = BinaryQuant,\n",
    "    output_scaling_impl=ParameterScaling,\n",
    "    output_scaling_init=1.0)\n",
    "\n",
    "inline_output_quant(torch.randn(1, 3, 5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the reason why, as it was mentioned in the first tutorial, quantized layers can accept arbitrary keyword arguments. It's really just a way to support different styles of syntax when defining a quantizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing a custom quantizer to QuantReLU\n",
    "\n",
    "We can do the same thing with quantized activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantReLU\n",
    "\n",
    "binary_relu = QuantReLU(act_quant=MySignedBinaryQuantizer)\n",
    "binary_relu(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there isn't really much difference between a quantizer for weights and a quantizer for activations. The difference with activations is that a prefix is not required when passing keyword arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0010, 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010, 0.0010],\n",
       "        [0.0010, 0.0010, 0.0010, 0.0010]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_scale_binary_relu = QuantReLU(act_quant=MySignedBinaryQuantizer, scaling_init=0.001)\n",
    "small_scale_binary_relu(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Custom Quantizer initialized with Weight Statistics\n",
    "\n",
    "So far we have seen use-cases where an `ExtendedInjector` provides, at best, a different kind of syntax to define a quantizer, without any particular other advantage. Let's now make things a bit more complicated to show the sort of situations where it really shines.\n",
    "\n",
    "Let's say we want to define a binary weight quantizer where `scaling_impl` is still `ParameterScaling`. However, instead of being user-defined, we want `scaling_init` to be the maximum value found in the weight tensor of the quantized layer.\n",
    "To support this sort of use cases where the quantizer depends on the layer, a quantized layer automatically passes itself to all its quantizers under the name of `module`.\n",
    "With only a few lines of code then, we can achieve our goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.inject import value\n",
    "\n",
    "class ParamFromMaxWeightQuantizer(MySignedBinaryQuantizer):\n",
    "    \n",
    "    @value\n",
    "    def scaling_init(module):\n",
    "        return module.weight.abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we are leveraging the `@value` *decorator* to define a function that is executed at *dependency-injection (DI) time*. This kind of behaviour is similar in spirit to defining a `@property` instead of an *attribute*, with the difference that a `@value` function can depend on other attributes of the Injector, which are automatically passed in as arguments of the function during DI.\n",
    "\n",
    "Let's now pass the quantizer to a QuantConv2d and retrieve its quantized weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.1912, -0.1912,  0.1912],\n",
       "          [ 0.1912, -0.1912, -0.1912],\n",
       "          [-0.1912, -0.1912,  0.1912]],\n",
       "\n",
       "         [[-0.1912, -0.1912, -0.1912],\n",
       "          [-0.1912,  0.1912,  0.1912],\n",
       "          [ 0.1912, -0.1912, -0.1912]],\n",
       "\n",
       "         [[ 0.1912,  0.1912,  0.1912],\n",
       "          [ 0.1912,  0.1912, -0.1912],\n",
       "          [-0.1912, -0.1912, -0.1912]]],\n",
       "\n",
       "\n",
       "        [[[-0.1912,  0.1912, -0.1912],\n",
       "          [ 0.1912,  0.1912, -0.1912],\n",
       "          [ 0.1912,  0.1912, -0.1912]],\n",
       "\n",
       "         [[-0.1912,  0.1912, -0.1912],\n",
       "          [-0.1912,  0.1912, -0.1912],\n",
       "          [ 0.1912, -0.1912, -0.1912]],\n",
       "\n",
       "         [[ 0.1912, -0.1912, -0.1912],\n",
       "          [ 0.1912,  0.1912,  0.1912],\n",
       "          [ 0.1912,  0.1912,  0.1912]]]], grad_fn=<MulBackward0>), scale=tensor(0.1912, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_from_max_quant_conv = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
    "param_from_max_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we can verify that `quant_weight_scale()` is equal to `weight.abs().max()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(param_from_max_quant_conv.quant_weight_scale() == param_from_max_quant_conv.weight.abs().max()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say now that we want to load a pretrained floating-point weight tensor on top of our quantized model. We simuate this scenario by defining a separate `nn.Conv2d` layer with the same weight shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1880, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "float_conv = nn.Conv2d(3, 2, (3, 3))\n",
    "float_conv.weight.abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we load it on top of `param_from_max_quant_conv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-22-5b3646241211>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mparam_from_max_quant_conv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfloat_conv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\pytorch_latest\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[1;34m(self, state_dict, strict)\u001B[0m\n\u001B[0;32m   1049\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1050\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1051\u001B[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[0m\u001B[0;32m   1052\u001B[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[0;32m   1053\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for QuantConv2d:\n\tMissing key(s) in state_dict: \"weight_quant.tensor_quant.scaling_impl.value\". "
     ]
    }
   ],
   "source": [
    "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch, we get an error. This is because `ParameterScaling` contains a learned `torch.nn.Parameter`, and Pytorch expects all learned parameters of a model to be contained in a `state_dict` that is being loaded.\n",
    "We can work around the issue by either setting the `IGNORE_MISSING_KEYS` config flag in Brevitas, or by passing `strict=False` to load_state_dict. We go with the former as setting `strict=False` is too forgiving to other kind of problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas import config\n",
    "config.IGNORE_MISSING_KEYS = True\n",
    "\n",
    "param_from_max_quant_conv.load_state_dict(float_conv.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could have also achieve the same goal by setting the *env variable* `BREVITAS_IGNORE_MISSING_KEYS=1`.\n",
    "\n",
    "And now if we take a look at the quantized weights again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantTensor(value=tensor([[[[-0.1880,  0.1880,  0.1880],\n",
       "          [-0.1880, -0.1880, -0.1880],\n",
       "          [ 0.1880,  0.1880, -0.1880]],\n",
       "\n",
       "         [[-0.1880, -0.1880,  0.1880],\n",
       "          [ 0.1880, -0.1880,  0.1880],\n",
       "          [-0.1880,  0.1880, -0.1880]],\n",
       "\n",
       "         [[ 0.1880,  0.1880,  0.1880],\n",
       "          [ 0.1880, -0.1880,  0.1880],\n",
       "          [-0.1880,  0.1880, -0.1880]]],\n",
       "\n",
       "\n",
       "        [[[-0.1880, -0.1880,  0.1880],\n",
       "          [-0.1880, -0.1880, -0.1880],\n",
       "          [ 0.1880, -0.1880,  0.1880]],\n",
       "\n",
       "         [[ 0.1880,  0.1880,  0.1880],\n",
       "          [ 0.1880,  0.1880,  0.1880],\n",
       "          [ 0.1880, -0.1880,  0.1880]],\n",
       "\n",
       "         [[-0.1880, -0.1880, -0.1880],\n",
       "          [ 0.1880, -0.1880,  0.1880],\n",
       "          [ 0.1880,  0.1880, -0.1880]]]], grad_fn=<MulBackward0>), scale=tensor(0.1880, grad_fn=<AbsBinarySignGradFnBackward>), zero_point=tensor(0.), bit_width=tensor(1.), signed=True, training=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_from_max_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, as expected, the scale factor has been updated to the new `weight.abs().max()`.\n",
    "\n",
    "What happens internally is that after `load_state_dict` is called on the layer, `ParamFromMaxWeightQuantizer.tensor_quant` gets called again to re-initialize `BinaryQuant`, and in turn `ParameterScaling` is re-initialized with a new `scaling_init` value computed based on the updated `module.weight` tensor. This whole process wouldn't have been possible without an `ExtendedInjector` behind it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing a Quantizer\n",
    "\n",
    "There are two ways to share a quantizer between multiple layers, with importance differences.\n",
    "\n",
    "The first one, which we have seen so far, is to simply pass the same ExtendedInjector to multiple layers. What that does is sharing the same quantization strategy among different layers. Each layer still gets its own instance of the quantization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryQuantizer)\n",
    "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryQuantizer)\n",
    "\n",
    "quant_conv1.weight_quant is quant_conv2.weight_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing an instance of Weight Quantization\n",
    "\n",
    "The second one, which we are introducing now, allows to share the same quantization instance among multiple layers. This can be useful in those scenarios where, for example, we want different layers to share the same scale factor. The syntax goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=MySignedBinaryQuantizer)\n",
    "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=quant_conv1.weight_quant)\n",
    "\n",
    "quant_conv1.weight_quant is quant_conv2.weight_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(quant_conv1.quant_weight_scale() == quant_conv2.quant_weight_scale()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens in background is that the weight quantizer now has access to both `quant_conv1` and `quant_conv2`. \n",
    "So let's say we want to build a quantizer similar to `ParamFromMaxWeightQuantizer`, but in this case we want the scale factor to be initialized with the average of both weight tensors. When a quantizer has access to multiple parent modules, they are passed in at dependency injection time as a *tuple* under the same name `module` as before. So we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SharedParamFromMeanWeightQuantizer(MySignedBinaryQuantizer):\n",
    "    \n",
    "    @value\n",
    "    def scaling_init(module):\n",
    "        if isinstance(module, tuple):\n",
    "            return torch.cat((module[0].weight.view(-1), module[1].weight.view(-1))).abs().mean()\n",
    "        else:\n",
    "            return module.weight.abs().mean()\n",
    "        \n",
    "quant_conv1 = QuantConv2d(3, 2, (3, 3), weight_quant=SharedParamFromMeanWeightQuantizer)\n",
    "old_quant_conv1_scale = quant_conv1.quant_weight_scale()\n",
    "quant_conv2 = QuantConv2d(3, 2, (3, 3), weight_quant=quant_conv1.weight_quant)\n",
    "new_quant_conv1_scale = quant_conv1.quant_weight_scale()\n",
    "\n",
    "(old_quant_conv1_scale == new_quant_conv1_scale).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_quant_conv1_scale == quant_conv2.quant_weight_scale()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how, when `quant_conv2` is initialized using the `weight_quant` of `quant_conv1`, weight quantization is re-initialized for both layers such that they end up having the same scale.\n",
    "\n",
    "We can see in this example how Brevitas works consistently with Pytorch's eager execution model. When we initialize `quant_conv1` we still don't know that its weight quantizer is going to be shared with `quant_conv2`, and the semantics of Pytorch impose that `quant_conv1` should work correctly both before and after `quant_conv2` is declared. The way we take advantage of dependency injection allows to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing an instance of Activation Quantization\n",
    "\n",
    "Sharing an instance of activation quantization is easier because for most scenarios it's enough to simply share the whole layer itself, e.g. calling the same `QuantReLU` from multiple places in the forward pass.\n",
    "\n",
    "For those scenarios where sharing the whole layer is not possible, there is something important to keep in mind. Instances of activation quantization include (for performance reasons) the implementation of the non-linear activation itself (if any). So, for example, using a `QuantReLU.act_quant` to initialize a `QuantConv2d.output_quant` should be avoided as we would not share not only the quantizer, but also the relu activation function.  \n",
    "In general then sharing of instances of activations quantization should be done only between activations of the same *kind*. \n",
    "\n",
    "*Note*: we say kind and not type because `input_quant`, `output_quant` and `IdentityQuant` count as being the same kind of activation, even though they belong to different type of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Weight Initialization\n",
    "\n",
    "There is a type of situation that Brevitas cannot deal with automatically. That is, when the initialization of the quantizer depends on the layer to which it is applied (like with the `ParamFromMaxWeightQuantizer` or `SharedParamFromMeanWeightQuantizer` quantizers), but the layer gets modified after it is initialized. \n",
    "\n",
    "The typical example is with weight initialization when training from scratch (so rather than loading from a floating-point state_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_conv_w_init = QuantConv2d(3, 2, (3, 3), weight_quant=ParamFromMaxWeightQuantizer)\n",
    "torch.nn.init.uniform_(quant_conv_w_init.weight)\n",
    "\n",
    "(quant_conv_w_init.weight.abs().max() == quant_conv_w_init.quant_weight_scale()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the scale factor is not initialized correctly anymore. In this case we can simply trigger re-initialization of the weight quantizer manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_conv_w_init.weight_quant.init_tensor_quant()\n",
    "\n",
    "(quant_conv_w_init.weight.abs().max() == quant_conv_w_init.quant_weight_scale()).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: because the way weights are initialized is often the same as to how an optimizer performs the weight update step, there are currently no plans to try to perform re-initialization automatically (as it happens e.g. when a `state_dict` is loaded) since it wouldn't be possible to distinguish between the two scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Custom Quantization API\n",
    "\n",
    "Finally, let's go through an even more complicated example. We are going to look at a scenario that illustrates the differences between a standard `Injector` (implemented in the dependencies library) and our `ExtendedInjector` extension.\n",
    "\n",
    "Let's say we want to build two quantizers for respectively weights and activations and build a simple API on top of them.\n",
    "In particular, we want to be able to switch between `BinaryQuant` and `ClampedBinaryQuant` (a variant of binary quantization with clamping), and we want to optionally perform *per-channel scaling*.\n",
    "To do so, we are going to implement the controlling logic through a hierarchy of ExtendedInjector, leaving two boolean flags exposed as arguments of the quantizers, with the idea then that the flags can be set through keyword arguments of the respective quantized layers.\n",
    "\n",
    "We can go as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.quant import ClampedBinaryQuant\n",
    "from brevitas.inject import this\n",
    "\n",
    "\n",
    "class CommonQuantizer(ExtendedInjector):\n",
    "    scaling_impl = ParameterScaling\n",
    "    signed=True\n",
    "    \n",
    "    @value\n",
    "    def tensor_quant(is_clamped):\n",
    "        # returning a class to auto-wire from a value function\n",
    "        # wouldn't be allowed in a standard Injector\n",
    "        if is_clamped:\n",
    "            return ClampedBinaryQuant\n",
    "        else:\n",
    "            return BinaryQuant\n",
    "    \n",
    "    @value\n",
    "    def scaling_shape(scaling_per_output_channel):\n",
    "        if scaling_per_output_channel:\n",
    "            # returning this.something from a value function \n",
    "            # wouldn't be allowed in a standard Injector\n",
    "            return this.per_channel_broadcastable_shape\n",
    "        else:\n",
    "            return ()\n",
    "        \n",
    "        \n",
    "class AdvancedWeightQuantizer(CommonQuantizer):\n",
    "        \n",
    "    @value\n",
    "    def per_channel_broadcastable_shape(module):\n",
    "        return (module.weight.shape[0], 1, 1, 1)\n",
    "    \n",
    "    @value\n",
    "    def scaling_init(module, scaling_per_output_channel):\n",
    "        if scaling_per_output_channel:\n",
    "            num_ch = module.weight.shape[0]\n",
    "            return module.weight.abs().view(num_ch, -1).max(dim=1)[0].view(-1, 1, 1, 1)\n",
    "        else:\n",
    "            return module.weight.abs().max()\n",
    "        \n",
    "        \n",
    "class AdvancedActQuantizer(CommonQuantizer):\n",
    "    scaling_init = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of things going on here to unpack.  \n",
    "\n",
    "The first one is that a `@value` function can return a class to auto-wire and inject, as seen in the definition of `tensor_quant`. This wouldn't normally be possible with a standard `Injector`, but it's possible with an `ExtendedInjector`. This way we can switch between different implementations of `tensor_quant`.\n",
    "\n",
    "The second one is the special object `this`. `this` is already present in the *dependencies* library, and it's used as a way to retrieve attributes of the quantizer from within the quantizer itself. However, normally it wouldn't be possible to return a reference to `this` from a `@value` function. Again this is something that only a `ExtendedInjector` supports, and it allows to chain different attributes in a way such that the chained values are computed only when necessary. \n",
    "\n",
    "Let's see the quantizers applied to a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_channel_quant_conv = QuantConv2d(\n",
    "    3, 2, (3, 3), \n",
    "    weight_quant=AdvancedWeightQuantizer, \n",
    "    weight_is_clamped=False, \n",
    "    weight_scaling_per_output_channel=True)\n",
    "per_channel_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the weight scale is now a vector. Everything we said so far about quantizers still applies, so for example we can load the floating-point state dict we defined before and observe how it triggers an update of the weight scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_channel_quant_conv.load_state_dict(float_conv.state_dict())\n",
    "per_channel_quant_conv.quant_weight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have a per-channel quantizer, so the original floating-point weight tensor is now quantized per channel. \n",
    "\n",
    "Similarly, we can apply our custom activation quantizer to e.g. a `QuantIdentity` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "quant_identity = QuantIdentity(\n",
    "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=False)\n",
    "quant_identity(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `AdvancedActQuantizer` doesn't define a `per_channel_broadcastable_shape`, yet no errors are triggered. This is because `this.per_channel_broadcastable_shape` is required only when `scaling_per_output_channel` is `True`, while in this case `scaling_per_output_channel` is `False`.\n",
    "Let' try to set it to `True` then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "quant_identity = QuantIdentity(\n",
    "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we get an error saying that the quantizer cannot resolve `per_channel_broadcastable_shape`. If we pass it in then we can get a per-channel quantizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_identity = QuantIdentity(\n",
    "    act_quant=AdvancedActQuantizer, is_clamped=True, scaling_per_output_channel=True,\n",
    "    per_channel_broadcastable_shape=(4, 1), return_quant_tensor=True)\n",
    "quant_identity(torch.randn(4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how powerful dependency injection is. In a way, it's even too expressive. For users that are not interesting in building completely custom quantizers, it can be hard to make sense of how the various components available under `brevitas.core` can be assembled together according to best practices.\n",
    "\n",
    "In the next tutorial then we are going to take a look at an API implemented with the mechanisms we just saw that builds on top of the components available in `brevitas.core` and exposes easily switchable settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}