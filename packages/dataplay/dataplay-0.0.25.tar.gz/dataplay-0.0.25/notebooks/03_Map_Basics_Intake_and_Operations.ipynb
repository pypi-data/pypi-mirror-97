{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3ZuFx1FD4o5"
   },
   "outputs": [],
   "source": [
    "# default_exp geoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mERqM1BxD4pD"
   },
   "source": [
    "# Geo-Data Intake and Operations\n",
    "\n",
    "> This notebook was made to demonstrate how to work with geographic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRZEDVAbD4pD"
   },
   "source": [
    "This Coding Notebook is the __third__ in a series.\n",
    "\n",
    "An Interactive version can be found here <a href=\"https://colab.research.google.com/github/karpatic/dataplay/blob/master/notebooks/03_Map_Basics_Intake_and_Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>.\n",
    "\n",
    "\n",
    "This colab and more can be found on our [webpage](https://karpatic.github.io/dataplay/). \n",
    "\n",
    "- Content covered in previous tutorials will be used in later tutorials. \n",
    "\n",
    "- __New code and or  information *should* have explanations and or descriptions__ attached. \n",
    "\n",
    "- Concepts or code covered in previous tutorials will be used without being explaining in entirety.\n",
    "\n",
    "- The [Dataplay](https://karpatic.github.io/dataplay/) Handbook development techniques covered in the [Datalabs](https://karpatic.github.io/datalabs/) Guidebook\n",
    "\n",
    "- __If content can not be found in the current tutorial and is not covered in previous tutorials, please let me know.__\n",
    "\n",
    "- This notebook has been optimized for Google Colabs ran on a Chrome Browser. \n",
    "\n",
    "- Statements found in the index page on view expressed, responsibility, errors and ommissions, use at risk, and licensing  extend throughout the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIo0gGqnD4pE"
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/karpatic/datalab/master?filepath=%2Fnotebooks%2Findex.ipynb)\r\n",
    "[![Binder](https://pete88b.github.io/fastpages/assets/badges/colab.svg)](https://colab.research.google.com/github/karpatic/datalab/blob/master/notebooks/index.ipynb)\r\n",
    "[![Binder](https://pete88b.github.io/fastpages/assets/badges/github.svg)](https://github.com/karpatic/datalab/tree/master/notebooks/index.ipynb)\r\n",
    "[![Open Source Love svg3](https://badges.frapsoft.com/os/v3/open-source.svg?v=103)](https://github.com/ellerbrock/open-source-badges/)\r\n",
    "\r\n",
    "[![NPM License](https://img.shields.io/npm/l/all-contributors.svg?style=flat)](https://github.com/karpatic/dataplay/blob/master/LICENSE)\r\n",
    "[![Active](http://img.shields.io/badge/Status-Active-green.svg)](https://karpatic.github.io) \r\n",
    "[![Python Versions](https://img.shields.io/pypi/pyversions/dataplay.svg)](https://pypi.python.org/pypi/dataplay/)\r\n",
    "[![GitHub last commit](https://img.shields.io/github/last-commit/karpatic/dataplay.svg?style=flat)]() \r\n",
    "[![No Maintenance Intended](http://unmaintained.tech/badge.svg)](http://unmaintained.tech/) \r\n",
    "\r\n",
    "[![GitHub stars](https://img.shields.io/github/stars/karpatic/dataplay.svg?style=social&label=Star)](https://github.com/karpatic/dataplay) \r\n",
    "[![GitHub watchers](https://img.shields.io/github/watchers/karpatic/dataplay.svg?style=social&label=Watch)](https://github.com/karpatic/dataplay) \r\n",
    "[![GitHub forks](https://img.shields.io/github/forks/karpatic/dataplay.svg?style=social&label=Fork)](https://github.com/karpatic/dataplay) \r\n",
    "[![GitHub followers](https://img.shields.io/github/followers/karpatic.svg?style=social&label=Follow)](https://github.com/karpatic/dataplay) \r\n",
    "\r\n",
    "[![Tweet](https://img.shields.io/twitter/url/https/github.com/karpatic/dataplay.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20%E2%9C%A8%20colab%20by%20@bniajfi%20https://github.com/karpatic/dataplay%20%F0%9F%A4%97) \r\n",
    "[![Twitter Follow](https://img.shields.io/twitter/follow/bniajfi.svg?style=social)](https://twitter.com/bniajfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyhZt05JD4pE"
   },
   "source": [
    "## About this Tutorial: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arpbdJBxD4pE"
   },
   "source": [
    "### Whats Inside?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL24bRrXD4pE"
   },
   "source": [
    "#### __The Tutorial__\n",
    "\n",
    "In this notebook, the basics of working with geographic data are introduced.\n",
    "\n",
    "- Reading in data (points/ geoms)\n",
    "-- Convert lat/lng columns to point coordinates\n",
    "-- Geocoding address to coordinates\n",
    "-- Changing coordinate reference systems\n",
    "-- Connecting to PostGisDB's\n",
    "- Basic Operations\n",
    "- Saving shape data\n",
    "- Get Polygon Centroids\n",
    "- Working with Points and Polygons\n",
    "-- Map Points and Polygons\n",
    "-- Get Points in Polygons\n",
    "-- Create Choropleths\n",
    "-- Create Heatmaps (KDE?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwJh9tekD4pE"
   },
   "source": [
    "#### __Objectives__\n",
    "\n",
    "By the end of this tutorial users should have an understanding of:\n",
    "- How to read in and process geo-data asa geo-dataframe.\n",
    "- The Coordinate Reference System and Coordinate Encoding\n",
    "- Basic geo-visualization strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaK2ZN58D4pF"
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w59CbWLD4pF"
   },
   "source": [
    "### Datatypes and Geo-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew-AqQziD4pF"
   },
   "source": [
    "Geographic data must be [encoded](https://www.blender.org/fileadmin/verse/spec/protocol-encoding.html#:~:text=3.3.-,Data%20Encoding,is%20specified%20in%20this%20section.) properly order to attain the full potential of the spatial nature of your geographic data.\n",
    "\n",
    "If you have read in a dataset using *pandas* it's data type will be a **Dataframe**.\n",
    "\n",
    "It may be converted into a **Geo-Dataframe** using *Geopandas* as demonstrated in the sections below. \n",
    "\n",
    "You can check a variables at any time using the [dtype]((https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html) command:\n",
    "\n",
    "```\n",
    "yourGeoDataframe.dtype\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr6_zP6OD4pF"
   },
   "source": [
    "## Coordinate Reference Systems (CRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQHjbTM7D4pF"
   },
   "source": [
    "**Make sure** the appropriate spatial *Coordinate Reference System* (CRS) is used when reading in your data!\n",
    "\n",
    "ala [wiki](https://en.wikipedia.org/wiki/Spatial_reference_system):\n",
    "> A spatial reference system (SRS) or coordinate reference system (CRS) is a coordinate-based local, regional or global system used to locate geographical entities\n",
    "\n",
    "**CRS 4326** is the CRS most people are familar with when refering to latiude and longitudes.\n",
    "\n",
    "Baltimore's 4326 CRS should be at (39.2, -76.6)\n",
    "\n",
    "BNIA uses [CRS 2248](http://www.spatialreference.org/ref/epsg/2248/) *internally*\n",
    "\n",
    "Additional Information: https://docs.qgis.org/testing/en/docs/gentle_gis_introduction/coordinate_reference_systems.html\n",
    "\n",
    "Ensure your geodataframes' coordinates are using the same CRS using the geopandas command: \n",
    "\n",
    "```\n",
    "yourGeoDataframe.CRS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZmmRrhLD4pF"
   },
   "source": [
    "## Coordinate Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5tLh3HTD4pH"
   },
   "source": [
    "When first recieving a spatial dataset, the spatial column may need to be encoded to convert its 'text' data type values into understood 'coordinate' data types before it can be understood/processed accordingly. \n",
    "\n",
    "Namely, there are two ways to encode text into coordinates: \n",
    "- df[geom] = df[geom].apply(lambda x: loads( str(x) ))\n",
    "- df[geom] = [Point(xy) for xy in zip(df.x, df.y)]\n",
    "\n",
    "The first approach can be used for text taking the form \"Point(-76, 39)\" and will encode the text too coordinates.\n",
    "The second approach is useful when creating a point from two columns containing lat/lng information and will create Point coordinates from the two columns.\n",
    "\n",
    "More on this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foDdQiH1D4pH"
   },
   "source": [
    "## Raster Vs Vector Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11ItQ_jgD4pH"
   },
   "source": [
    "There exists two types of Geospatial Data, Raster and Vector. \n",
    "Both have different file formats.\n",
    "\n",
    "This lab will only cover vector data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EDq89CKD4pH"
   },
   "source": [
    "### Vector Data \n",
    "Vector Data: Individual points stored as (x,y) coordinates pairs. These points can be joined to create lines or polygons.\n",
    "\n",
    "Format of Vector data\n",
    "\n",
    "Esri Shapefile â€” .shp, .dbf, .shx\n",
    "Description - Industry standard, most widely used. The three files listed above are needed to make a shapefile. Additional file formats may be included.\n",
    "\n",
    "Geographic JavaScript Object Notation â€” .geojson, .json\n",
    "Description â€” Second most popular, Geojson is typically used in web-based mapping used by storing the coordinates as JSON.\n",
    "\n",
    "Geography Markup Language â€” .gml\n",
    "Description â€” Similar to Geojson, GML has more data for the same amount of information.\n",
    "\n",
    "Google Keyhole Markup Language  â€” .kml, .kmz\n",
    "Description â€” XML-based and predominantly used for google earth. KMZ is a the newer, zipped version of KML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZs9HqbRD4pI"
   },
   "source": [
    "### Raster Data \n",
    "Raster Data: Cell-based data where each cell represent geographic information. An Aerial photograph is one such example where each pixel has a color value\n",
    "\n",
    "Raster Data Files: \n",
    "GeoTIFF â€” .tif, .tiff, .ovr\n",
    "ERDAS Imagine â€” .img\n",
    "IDRISI Raster â€” .rst, .rdc\n",
    "\n",
    "\n",
    "Information Sourced From: https://towardsdatascience.com/getting-started-with-geospatial-works-1f7b47955438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wtRtydqD4pI"
   },
   "source": [
    "**Vector Data: Census Geographic Data**:\n",
    "- **Geographic Coordinate Data** is provided by the census and compliments their census geographies \n",
    "- https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2010.html\n",
    "- https://www.census.gov/programs-surveys/acs/geography-acs/geography-boundaries-by-year.html \n",
    "- Bnia created and provides for free geographic boundary data that compliment these CSA's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVW8WL5gD4pJ"
   },
   "source": [
    "# Guided Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxOwMSSOD4pJ"
   },
   "source": [
    "## SETUP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oavXrxlcD4pJ"
   },
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_D7ELBdD4pJ"
   },
   "outputs": [],
   "source": [
    "# @title Run: Install Modules\r\n",
    "%%capture\r\n",
    "! pip install geopy\r\n",
    "! pip install geopandas\r\n",
    "! pip install geoplot\r\n",
    "! pip install dataplay\r\n",
    "! pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oXoKpdvSFG-"
   },
   "outputs": [],
   "source": [
    "from dataplay import geoms\r\n",
    "from dataplay import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDDJDaKtD4pJ"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# @title Run: Import Modules\n",
    "\n",
    "# These imports will handle everything\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "# conda install -c conda-forge proj4\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import LineString\n",
    "from shapely import wkb\n",
    "from shapely.wkt import loads\n",
    "# https://pypi.org/project/geopy/\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "# In case file is KML, enable support\n",
    "import fiona\n",
    "fiona.drvsupport.supported_drivers['kml'] = 'rw'\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6pOnISWD4pJ"
   },
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liupTKfCD4pK"
   },
   "source": [
    "### Configure Enviornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIpv4s98D4pK"
   },
   "outputs": [],
   "source": [
    "# This will just beautify the output\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 2)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# pd.set_option('display.expand_frame_repr', False)\n",
    "# pd.set_option('display.precision', 2)\n",
    "# pd.reset_option('max_colwidth')\n",
    "pd.set_option('max_colwidth', 50)\n",
    "# pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UrQrd4AD4pK"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "# %matplotlib inline\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB7dkxCSD4pK"
   },
   "source": [
    "## Retrieve GIS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C5QlIrpD4pK"
   },
   "source": [
    "As mentioned earlier: \r\n",
    "\r\n",
    "When you use a pandas function to 'read-in' a dataset, the returned value is of a datatype called a 'Dataframe'. \r\n",
    "\r\n",
    "We need a 'Geo-Dataframe', however, to effectively work with spatial data. \r\n",
    "\r\n",
    "While Pandas does not support Geo-Dataframes; Geo-pandas does. \r\n",
    "\r\n",
    "Geopandas has everything you love about pandas, but with added support for geo-spatial data.\r\n",
    "\r\n",
    "Principle benefits of using Geopandas over Pandas when working with spatial data: \r\n",
    "- The geopandas plot function will now render a map by default using your 'spatial-geometries' column.\r\n",
    "- Libraries exist spatial-operations and interactive map usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsI5AIegD4pK"
   },
   "source": [
    "There are many ways to have our spatial-data be read-in using geo-pandas into a geo-dataframe.\r\n",
    "\r\n",
    "Namely, it means reading in Geo-Spatial-data from a:\r\n",
    "1. (.geojson or .shp) file directly using Geo-pandas\r\n",
    "2. (.csv, .json) file using Pandas and convert it to Geo-Pandas \r\n",
    "  - using a prepared 'geometry' column\r\n",
    "  - by transformting latitude and longitude columns into a 'geometry' column.\r\n",
    "  - acquiring coordinates from an address\r\n",
    "  - mapping your non-spatial-data to data-with-space \r\n",
    "3. Connecting to a DB\r\n",
    "\r\n",
    "We will review each one below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3-rZlGgD4pK"
   },
   "source": [
    "### Approach 1: Reading in Data Directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nalTsyrzD4pL"
   },
   "source": [
    "If you are using Geopandas, direct imports **only work** with geojson and shape files. \r\n",
    "\r\n",
    "spatial coordinate data is properly encoded with these types of files soas to make them particularly easy to use.\r\n",
    "\r\n",
    "You can perform this using geopandas' `read_file()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et7Vbss7D4pL"
   },
   "outputs": [],
   "source": [
    "# This dataset is taken from the public database provided by BNIAJFI hosted by Esri / ArcGIS\n",
    "# BNIA ArcGIS Homepage: https://data-bniajfi.opendata.arcgis.com/\n",
    "csa_gdf = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Hhchpov/FeatureServer/1/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlRKEStgD4pL"
   },
   "source": [
    "As you can see, the resultant variable is of type GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHDYYIUYD4pL"
   },
   "outputs": [],
   "source": [
    "type(csa_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os4mBIwLD4pL"
   },
   "source": [
    "GeoDataFrames are only possible when one of the columns are of a 'Geometry' Datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjiVZp-pD4pL"
   },
   "outputs": [],
   "source": [
    "csa_gdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RveEjQ-ZD4pL"
   },
   "source": [
    "Awesome. So that means, now you can plot maps all prety like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIe1gMv3D4pM"
   },
   "outputs": [],
   "source": [
    "csa_gdf.plot(column='hhchpov15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3B6_F-yD4pM"
   },
   "source": [
    "And now lets take a peak at the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDARH5kyD4pM"
   },
   "outputs": [],
   "source": [
    "csa_gdf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlEP3JnRD4pM"
   },
   "source": [
    "I'll show you more ways to save the data later, but for our example in the next section to work, we need a csv. \r\n",
    "\r\n",
    "We can make one by saving the geo-dataframe avove using the `to_gdf` function.\r\n",
    "\r\n",
    "The spatial data will be stored in an encoded form that will make it easy to re-open up in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wW7RA_PD4pM"
   },
   "outputs": [],
   "source": [
    "csa_gdf.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-a6CynvD4pM"
   },
   "source": [
    "### Approach 2: Converting Pandas into Geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQuuzgZeD4pM"
   },
   "source": [
    "#### Approach 2: Method 1: Convert using a pre-formatted 'geometry' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLrz6907D4pM"
   },
   "source": [
    "This approach loads a map using a geometry column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k75DiY1oD4pN"
   },
   "source": [
    "In our previous example, we saved a geo-dataframe as a csv. \r\n",
    "\r\n",
    "Now lets re-open it up using pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkUuR4lmD4pN"
   },
   "outputs": [],
   "source": [
    "# A url to a public Dataset.\r\n",
    "url = \"example.csv\"\r\n",
    "geom = 'geometry'\r\n",
    "# An example of loading in an internal BNIA file\r\n",
    "crs = {'init' :'epsg:2248'} \r\n",
    "\r\n",
    "# Read in the dataframe\r\n",
    "csa_gdf = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJEVRY1eD4pN"
   },
   "source": [
    "Great!\r\n",
    "\r\n",
    "But now what?\r\n",
    "\r\n",
    "Well, for starters, regardless of the project you are working on: It's always a good idea to inspect your data. \r\n",
    "\r\n",
    "This is particularly important if you don't know what you're working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWQtxfBgD4pN"
   },
   "outputs": [],
   "source": [
    "csa_gdf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYUPR7WMD4pN"
   },
   "source": [
    "Take notice of how the geometry column has a special.. foramatting. \r\n",
    "\r\n",
    "All spatial data must take on a similar form encoding for it to be properly interpretted as a spatial data-type. \r\n",
    "\r\n",
    "As far as I can tell, This is near-identical to the table I printed out in our last example. \r\n",
    "\r\n",
    "BUT WAIT!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqFQN757D4pN"
   },
   "source": [
    "You'll notice, that if I run the plot function a pretty map will not de-facto appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CqpyqYm3D4pN"
   },
   "outputs": [],
   "source": [
    "csa_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFIH5sUmD4pN"
   },
   "source": [
    "Why is this? Because you're not working with a geo-dataframe but just a dataframe!\r\n",
    "\r\n",
    "Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e29E5tXaD4pO"
   },
   "outputs": [],
   "source": [
    "type(csa_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eQPvqqmD4pO"
   },
   "source": [
    "Okay... So thats not right.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5utlBtfD4pO"
   },
   "source": [
    "What can we do about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLGiCNE_D4pO"
   },
   "source": [
    "Well for one, our spatial data (in the geometry-column) is not of the right data-type even though it takes on the right form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sElB4gotD4pO"
   },
   "outputs": [],
   "source": [
    "csa_gdf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fExLbOVRD4pP"
   },
   "source": [
    "Ok. So how do we change it? Well since it's already been properly encoded... You can convert a columns data-type from an object (or whatver) to a 'geometry' using the `loads` function. \r\n",
    "\r\n",
    "In the example below, we convert the datatypes for all records in the 'geometry' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGmwPTYoD4pP"
   },
   "outputs": [],
   "source": [
    "# Convert the geometry column datatype from a string of text into a coordinate datatype\n",
    "csa_gdf[geom] = csa_gdf[geom].apply(lambda x: loads( str(x) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqILvoTvD4pP"
   },
   "source": [
    "Thats all! Now lets see the geometry columns data-type and the entire tables's data-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02um39deD4pP"
   },
   "outputs": [],
   "source": [
    "csa_gdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2FS6jSiD4pQ"
   },
   "outputs": [],
   "source": [
    "type(csa_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOffnbo2D4pQ"
   },
   "source": [
    "As you can see, we have a geometry column of the right datatype, but our table is still only just a dataframe. \r\n",
    "\r\n",
    "But now, you are ready to convert your entire pandas dataframe into a geo-dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbFCV1OdD4pQ"
   },
   "source": [
    "You can do that by running the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmLJ4l5WD4pQ"
   },
   "outputs": [],
   "source": [
    "# Process the dataframe as a geodataframe with a known CRS and geom column\r\n",
    "csa_gdf = GeoDataFrame(csa_gdf, crs=crs, geometry=geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYIu96T8D4pQ"
   },
   "source": [
    "Aaaand BOOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdxuryZQD4pQ"
   },
   "outputs": [],
   "source": [
    "csa_gdf.plot(column='hhchpov18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7byetXhD4pQ"
   },
   "source": [
    "goes the dy-no-mite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqN_wMnLD4pR"
   },
   "outputs": [],
   "source": [
    "type(csa_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50pDP3wkD4pR"
   },
   "source": [
    "#### Approach 2: Method 2: Convert Column(s) to Coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiZrbx2cD4pR"
   },
   "source": [
    "##### Approach 2: Method 1:  A Generic Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIu5KyJjD4pR"
   },
   "source": [
    "This is the generic example but it will not work since no URL is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-WScwfdD4pR"
   },
   "outputs": [],
   "source": [
    "# More Information: https://geopandas.readthedocs.io/en/latest/gallery/create_geopandas_from_pandas.html#from-longitudes-and-latitudes\n",
    "\n",
    "# If your data has coordinates in two columns run this cell\n",
    "# It will create a geometry column from the two.\n",
    "# A public dataset is not provided for this example and will not run.\n",
    "\n",
    "# Load DF HERE. Accidently deleted the link. Need to refind. \n",
    "# Just rely on example 2 for now. \n",
    "\"\"\"\n",
    "exe_df['x'] = pd.to_numeric(exe_df['x'], errors='coerce')\n",
    "exe_df['y'] = pd.to_numeric(exe_df['y'], errors='coerce')\n",
    "# exe_df = exe_df.replace(np.nan, 0, regex=True)\n",
    "\n",
    "# An example of loading in an internal BNIA file\n",
    "geometry=[Point(xy) for xy in zip(exe_df.x, exe_df.y)]\n",
    "exe_gdf = gpd.GeoDataFrame( exe_df.drop(['x', 'y'], axis=1), crs=crs, geometry=geometry)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzLSDjdED4pR"
   },
   "source": [
    "##### Approach 2: Method 2:  Example: Geoloom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1Q7gsfDD4pR"
   },
   "source": [
    "Since I do not readily have a dataset with lat and long's I will have to make one.\r\n",
    "\r\n",
    "We can split the coordinates from a geodataframe like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZAbSXRVzD4pR"
   },
   "outputs": [],
   "source": [
    "# Alternate Primary Table\n",
    "# Table: Geoloom, \n",
    "# Columns:  \n",
    "geoloom_gdf = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Geoloom_Crowd/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\n",
    "geoloom_gdf['POINT_X'] = geoloom_gdf['geometry'].centroid.x\n",
    "geoloom_gdf['POINT_Y'] = geoloom_gdf['geometry'].centroid.y\n",
    "# Now lets just drop the geometry column and save it to have our example dataset. \n",
    "geoloom_gdf = geoloom_gdf.dropna(subset=['geometry'])\n",
    "geoloom_gdf.to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFFOuhNnD4pS"
   },
   "source": [
    "The first thing you will want to do when given a dataset with a coordinates column is ensure its datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGH3o_Y8D4pS"
   },
   "outputs": [],
   "source": [
    "geoloom_df = pd.read_csv('example.csv')\r\n",
    "# We already know the x and y columns because we just saved them as such.\r\n",
    "geoloom_df['POINT_X'] = pd.to_numeric(geoloom_df['POINT_X'], errors='coerce')\r\n",
    "geoloom_df['POINT_Y'] = pd.to_numeric(geoloom_df['POINT_Y'], errors='coerce')\r\n",
    "# df = df.replace(np.nan, 0, regex=True)\r\n",
    "\r\n",
    "# And filter out for points only in Baltimore City. \r\n",
    "geoloom_df = geoloom_df[ geoloom_df['POINT_Y'] > 39.3  ]\r\n",
    "geoloom_df = geoloom_df[ geoloom_df['POINT_Y'] < 39.5  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqIHWikvD4pS"
   },
   "outputs": [],
   "source": [
    "# An example of loading in an internal BNIA file\n",
    "crs = {'init' :'epsg:2248'} \n",
    "geometry=[Point(xy) for xy in zip(geoloom_df['POINT_X'], geoloom_df['POINT_Y'])]\n",
    "geoloom_gdf = gpd.GeoDataFrame( geoloom_df.drop(['POINT_X', 'POINT_Y'], axis=1), crs=crs, geometry=geometry)\n",
    "# 39.2904Â° N, 76.6122Â°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA_LTxmHD4pS"
   },
   "outputs": [],
   "source": [
    "geoloom_gdf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KharqdywD4pT"
   },
   "source": [
    "Heres a neat trick to make it more presentable, because those points mean nothing to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFmheTBAD4pT"
   },
   "outputs": [],
   "source": [
    "# Create our base layer.\r\n",
    "ax = csa_gdf.plot(column='hhchpov18', edgecolor='black')\r\n",
    "\r\n",
    "# now plot our points over it.\r\n",
    "geoloom_gdf.plot(ax=ax, color='red')\r\n",
    "\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX4ZxzKhD4pU"
   },
   "source": [
    "##### Approach 2: Method 3: Using a Crosswalk (Need Crosswalk on Esri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwJWDZAOD4pU"
   },
   "source": [
    "When you want to merge two datasets that do not share a common column, it is often useful to create a 'crosswalk' file that 'maps' records between two datasets. We can do this to append spatial data when a direct merge is not readily evident. \r\n",
    "\r\n",
    "Check out this next example where we pull ACS Census data and use its 'tract' column and map it to a community. We can then aggregate the points along a the communities they belong to and map it on a choropleth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TNhBD1MD4pU"
   },
   "source": [
    "We will set up our ACS query variables right here for easy changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZxM3ftHD4pU"
   },
   "outputs": [],
   "source": [
    "# Our download function will use Baltimore City's tract, county and state as internal paramters\r\n",
    "# Change these values in the cell below using different geographic reference codes will change those parameters\r\n",
    "tract = '*'\r\n",
    "county = '510' # '059' # 153 '510'\r\n",
    "state = '24' #51\r\n",
    "\r\n",
    "# Specify the download parameters the function will receieve here\r\n",
    "tableId = 'B19049' # 'B19001'\r\n",
    "year = '17'\r\n",
    "saveAcs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvwbmL7iD4pU"
   },
   "source": [
    "And now we will call the function with those variables and check out the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em9pi3MZD4pU"
   },
   "outputs": [],
   "source": [
    "import dataplay\n",
    "from dataplay import acsDownload\n",
    "import IPython\n",
    "retrieve_acs_data = acsDownload.retrieve_acs_data\n",
    "# from IPython.core.display import HTML\n",
    "IPython.core.display.HTML(\"<style>.rendered_html th {max-width: 200px; overflow:auto;}</style>\")\n",
    "# state, county, tract, tableId, year, saveOriginal, save \n",
    "df = retrieve_acs_data(state, county, tract, tableId, year, saveAcs)\n",
    "df.head(1)\n",
    "df.to_csv('tracts_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hm6_vOTiD4pV"
   },
   "outputs": [],
   "source": [
    "df['tract'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuftpOQ_D4pV"
   },
   "source": [
    "As you can see, the tract column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ge14SVb2D4pV"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcjUcSYaD4pV"
   },
   "outputs": [],
   "source": [
    "!wget https://bniajfi.org/vs_resources/CSA-to-Tract-2010.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sir5Fp9AD4pV"
   },
   "outputs": [],
   "source": [
    "df['tract'].tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJmV4n8AD4pV"
   },
   "outputs": [],
   "source": [
    "crosswalk = pd.read_csv('CSA-to-Tract-2010.csv')\r\n",
    "crosswalk.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFfdXQLbD4pV"
   },
   "outputs": [],
   "source": [
    "Hhchpov = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Hhchpov/FeatureServer/1/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\")\r\n",
    "Hhchpov = Hhchpov[['CSA2010', 'hhchpov15',\t'hhchpov16',\t'hhchpov17',\t'hhchpov18', 'geometry']]\r\n",
    "Hhchpov.to_file(\"Hhchpov.geojson\", driver='GeoJSON')\r\n",
    "Hhchpov.to_csv('Hhchpov.csv')\r\n",
    "gpd.read_file(\"Hhchpov.geojson\").head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahDlGN4PCPDH"
   },
   "source": [
    "A simple example of how this would work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_R8SVDsD4pW"
   },
   "outputs": [],
   "source": [
    "# A simple merge\r\n",
    "df.merge(crosswalk, left_on='tract', right_on='TRACTCE10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qksAzIatD4pW"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66YU76d9FCiZ"
   },
   "outputs": [],
   "source": [
    "# geoms.readInGeometryData(url='Hhchpov.geojson').head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GOqo_VcD4pW"
   },
   "outputs": [],
   "source": [
    "\n",
    "# The attributes are what we will use.\n",
    "in_crs = 2248 # The CRS we recieve our data \n",
    "out_crs = 4326 # The CRS we would like to have our data represented as\n",
    "geom = 'geometry' # The column where our spatial information lives.\n",
    "\n",
    "# To create this dataset I had to commit a full outer join. \n",
    "# In this way geometries will be included even if there merge does not have a direct match. \n",
    "# What this will do is that it means at least one (near) empty record for each community will exist that includes (at minimum) the geographic information and name of a Community.\n",
    "# That way if no point level information existed in the community, that during the merge the geoboundaries are still carried over.\n",
    "\n",
    "# Primary Table\n",
    "# Description: I created a public dataset from a google xlsx sheet 'Bank Addresses and Census Tract'.\n",
    "# Table: FDIC Baltimore Banks\n",
    "# Columns: Bank Name, Address(es), Census Tract\n",
    "left_ds = 'tracts_data.csv'\n",
    "left_col = 'tract'\n",
    "\n",
    "# Crosswalk Table\n",
    "# Table: Crosswalk Census Communities\n",
    "# 'TRACT2010', 'GEOID2010', 'CSA2010'\n",
    "crosswalk_ds = 'CSA-to-Tract-2010.csv'\n",
    "use_crosswalk = True\n",
    "crosswalk_left_col = 'TRACTCE10'\n",
    "crosswalk_right_col = 'CSA2010'\n",
    "\n",
    "# Secondary Table\n",
    "# Table: Baltimore Boundaries => HHCHPOV\n",
    "# 'TRACTCE10', 'GEOID10', 'CSA', 'NAME10', 'Tract', 'geometry'\n",
    "right_ds = 'Hhchpov.geojson'\n",
    "right_col ='CSA2010'\n",
    "\n",
    "interactive = True\n",
    "merge_how = 'outer'\n",
    "\n",
    "# reutrns a pandas dataframe\n",
    "mergedf = merge.mergeDatasets( left_ds=left_ds, left_col=left_col, \n",
    "              crosswalk_ds=crosswalk_ds,\n",
    "              crosswalk_left_col = crosswalk_left_col, crosswalk_right_col = crosswalk_right_col,\n",
    "              right_ds=right_ds, right_col=right_col, \n",
    "              merge_how=merge_how, interactive = interactive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEqvE3bvD4pW"
   },
   "outputs": [],
   "source": [
    "mergedf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wY2dPSIHD4pW"
   },
   "outputs": [],
   "source": [
    "# Convert the geometry column datatype from a string of text into a coordinate datatype\r\n",
    "# mergedf[geom] = mergedf[geom].apply(lambda x: loads( str(x) ) ) \r\n",
    "\r\n",
    "# Process the dataframe as a geodataframe with a known CRS and geom column \r\n",
    "mergedGdf = GeoDataFrame(mergedf, crs=in_crs, geometry=geom) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USuQ3Af-D4pX"
   },
   "outputs": [],
   "source": [
    "mergedGdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVvU32H0D4pX"
   },
   "source": [
    "##### Approach 2: Method 4: Geocoding Addresses and Landmarks to Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YieYhJV5D4pX"
   },
   "source": [
    "Sometimes (usually) we just don't have the coordinates of a place, but we do know it's address or that it is an established landmark. \r\n",
    "\r\n",
    "In such cases we attempt 'geo-coding' these points in an automated manner.\r\n",
    "\r\n",
    "While convenient, this process is error prone, so be sure to check it's work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVUzEN58D4pX"
   },
   "source": [
    "For this next example to take place, we need a dataset that has a bunch of addresses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYZvJ_-zD4pX"
   },
   "source": [
    "We can use the geoloom dataset from before in this example. We'll just drop geo'spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5LhJGwiD4pX"
   },
   "outputs": [],
   "source": [
    "geoloom = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Geoloom_Crowd/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\r\n",
    "geoloom = geoloom.dropna(subset=['geometry'])\r\n",
    "geoloom = geoloom.drop(columns=['geometry','GlobalID', 'POINT_X',\t'POINT_Y'])\r\n",
    "geoloom.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5C9SUErD4pX"
   },
   "source": [
    "But if for whatever reason the link is down, you can use this example dataframe mapping just some of the many malls in baltimore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54M4t2hdD4pX"
   },
   "outputs": [],
   "source": [
    "address_df = pd.DataFrame({ \r\n",
    "    'Location' : pd.Series([\r\n",
    "    '100 N. Holliday St, Baltimore, MD 21202',\r\n",
    "    '200 E Pratt St, Baltimore, MD',\r\n",
    "    '2401 Liberty Heights Ave, Baltimore, MD',\r\n",
    "    '201 E Pratt St, Baltimore, MD',\r\n",
    "    '3501 Boston St, Baltimore, MD',\r\n",
    "    '857 E Fort Ave, Baltimore, MD',\r\n",
    "    '2413 Frederick Ave, Baltimore, MD'\r\n",
    "  ]),\r\n",
    "    'Address' : pd.Series([ \r\n",
    "    'Baltimore City Council',\r\n",
    "    'The Gallery at Harborplace',\r\n",
    "    'Mondawmin Mall',\r\n",
    "    'Harborplace',\r\n",
    "    'The Shops at Canton Crossing',\r\n",
    "    'Southside Marketplace',\r\n",
    "    'Westside Shopping Center'\r\n",
    "  ])\r\n",
    "})\r\n",
    "\r\n",
    "address_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2f3tIeTD4pY"
   },
   "source": [
    "You can use either the Location or Address column to perform the geo-coding on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhXXqx6SD4pY"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "address_df = geoloom.copy()\r\n",
    "addrCol = 'Location'\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY1d8a_nD4pY"
   },
   "source": [
    "This function takes a while. The less columns/data/records the faster it executes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTWzTDqRD4pY"
   },
   "outputs": [],
   "source": [
    "# More information vist: https://geopy.readthedocs.io/en/stable/#module-geopy.geocoders\n",
    "\n",
    "# In this example we retrieve and map a dataset with no lat/lng but containing an address\n",
    "\n",
    "# In this example our data is stored in the 'STREET' attribute\n",
    "geometry = []\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "\n",
    "for index, row in address_df.iterrows():\n",
    "  # We will try and return an address for each Street Name\n",
    "  try: \n",
    "      # retrieve the geocoded information of our street address\n",
    "      geol = geolocator.geocode(row[addrCol], timeout=None)\n",
    "\n",
    "      # create a mappable coordinate point from the response object's lat/lang values.\n",
    "      pnt = Point(geol.longitude, geol.latitude)\n",
    "      \n",
    "      # Append this value to the list of geometries\n",
    "      geometry.append(pnt)\n",
    "      \n",
    "  except: \n",
    "      # If no street name was found decide what to do here.\n",
    "      # df.loc[index]['geom'] = Point(0,0) # Alternate method\n",
    "      geometry.append(Point(0,0))\n",
    "      \n",
    "# Finally, we stuff the geometry data we created back into the dataframe\n",
    "address_df['geometry'] = geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lXeC-SMD4pY"
   },
   "outputs": [],
   "source": [
    "address_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnJ89B1CD4pY"
   },
   "source": [
    "Awesome! Now convert the dataframe into a geodataframe and map it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y0-JMxtD4pY"
   },
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame( address_df, geometry=geometry)\r\n",
    "gdf = gdf[ gdf.centroid.y > 39.3  ]\r\n",
    "gdf = gdf[ gdf.centroid.y < 39.5  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1VG-XS4D4pY"
   },
   "outputs": [],
   "source": [
    "# Create our base layer.\r\n",
    "ax = csa_gdf.plot(column='hhchpov18', edgecolor='black')\r\n",
    "\r\n",
    "# now plot our points over it.\r\n",
    "geoloom_gdf.plot(ax=ax, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpf5qRZxD4pZ"
   },
   "source": [
    "A litte later down, we'll see how to make this even-more interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkthVaX4D4pZ"
   },
   "source": [
    "### Approach 3: Connecting to a PostGIS database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYnT2XrkD4pZ"
   },
   "source": [
    "In the following example pulls point geodata from a Postgres database.\n",
    "\n",
    "We will pull the postgres point data in two manners. \n",
    "- SQL query where an SQL query uses ST_Transform(the_geom,4326) to transform the_geom's CRS from a DATABASE Binary encoding into standard Lat Long's\n",
    "- Using a plan SQL query and performing the conversion using gpd.io.sql.read_postgis() to pull the data in as 2248 and convert the CRS using .to_crs(epsg=4326)\n",
    "- These examples will not work in colabs as their is no local database to connect to and has been commented out for that reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WGn0MnpD4pZ"
   },
   "outputs": [],
   "source": [
    "# This Notebook can be downloaded to connect to a database\n",
    "'''\n",
    "conn = psycopg2.connect(host='', dbname='', user='', password='', port='')\n",
    "\n",
    "# DB Import Method One\n",
    "sql1 = 'SELECT the_geom, gid, geogcode, ooi, address, addrtyp, city, block, lot, desclu, existing FROM housing.mdprop_2017v2 limit 100;'\n",
    "pointData = gpd.io.sql.read_postgis(sql1, conn, geom_col='the_geom', crs=2248)\n",
    "pointData = pointData.to_crs(epsg=4326)\n",
    "\n",
    "# DB Import Method Two\n",
    "sql2 = 'SELECT ST_Transform(the_geom,4326) as the_geom, ooi, desclu, address FROM housing.mdprop_2017v2;'\n",
    "pointData = gpd.GeoDataFrame.from_postgis(sql2, conn, geom_col='the_geom', crs=4326)\n",
    "pointData.head()\n",
    "pointData.plot()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bvvw02BD4pZ"
   },
   "source": [
    "## Basics Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxkgDP78D4pZ"
   },
   "source": [
    "### Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOnHk2XqD4pZ"
   },
   "outputs": [],
   "source": [
    "def geomSummary(gdf): return type(gdf), gdf.crs, gdf.columns;\n",
    "# for p in df['Tract'].sort_values(): print(p)\n",
    "geomSummary(csa_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaqT79gqD4pZ"
   },
   "source": [
    "### Converting CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3f_JkRHD4pa"
   },
   "outputs": [],
   "source": [
    "# Convert the CRS of the dataset into one you desire\n",
    "# The gdf must be loaded with a known crs in order for the to_crs conversion to work\n",
    "# We use this often to converting BNIAs custom CRS to the common type \n",
    "out_crs = 4326\n",
    "csa_gdf = csa_gdf.to_crs(epsg=out_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk3Yut2hD4pb"
   },
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHlwO8B_D4pb"
   },
   "outputs": [],
   "source": [
    "# Here is code to comit a simple save\n",
    "filename = 'TEST_FILE_NAME'\n",
    "csa_gdf.to_file(f\"{filename}.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya0vaN_JD4pc"
   },
   "outputs": [],
   "source": [
    "# Here is code to save this new projection as a geojson file and read it back in\n",
    "csa_gdf = csa_gdf.to_crs(epsg=2248) #just making sure\n",
    "csa_gdf.to_file(filename+'.shp', driver='ESRI Shapefile')\n",
    "csa_gdf = gpd.read_file(filename+'.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4DdsC13D4pc"
   },
   "source": [
    "### Geometric Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtrKbZJ_D4pc"
   },
   "source": [
    "Draw Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2jrQgPmD4pc"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import Draw\n",
    "# Draw tool. Create and export your own boundaries\n",
    "m = folium.Map()\n",
    "draw = Draw()\n",
    "draw.add_to(m)\n",
    "m = folium.Map(location=[-27.23, -48.36], zoom_start=12)\n",
    "draw = Draw(export=True)\n",
    "draw.add_to(m)\n",
    "# m.save(os.path.join('results', 'Draw1.html'))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13bayqICD4pc"
   },
   "source": [
    "Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPh1tIZRD4pc"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.boundary\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYs6ezIED4pc"
   },
   "source": [
    "envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYNISJCGD4pd"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.envelope\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q48Do_9GD4pd"
   },
   "source": [
    "convex_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sra3nFBZD4pd"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.convex_hull\n",
    "newcsa.plot(column='CSA2010' )\n",
    "# , cmap='OrRd', scheme='quantiles'\n",
    "# newcsa.boundary.plot(  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfa4twIvD4pd"
   },
   "source": [
    "simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebHZ9bLeD4pd"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.simplify(30)\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ch7vgz_iD4pd"
   },
   "source": [
    "buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIvWoXz2D4pd"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.buffer(0.01)\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs5NseQHD4pe"
   },
   "source": [
    "rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csIROOxwD4pe"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.rotate(30)\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKptUnDxD4pe"
   },
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKyGeh5ZD4pe"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.scale(3, 2)\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFkHZ928D4pe"
   },
   "source": [
    "skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Hh9zb1TD4pe"
   },
   "outputs": [],
   "source": [
    "newcsa = csa_gdf.copy()\n",
    "newcsa['geometry'] = csa_gdf.skew(1, 10)\n",
    "newcsa.plot(column='CSA2010' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_s8EWKnD4pe"
   },
   "source": [
    "# Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9VE2E17D4pf"
   },
   "source": [
    "## Create Geospatial Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuiAUKynD4pf"
   },
   "source": [
    "**Operations:** \n",
    "\n",
    "- Reading in data (points/ geoms)\n",
    "-- Convert lat/lng columns to point coordinates\n",
    "-- Geocoding address to coordinates\n",
    "-- Changing coordinate reference systems\n",
    "-- Connecting to PostGisDB's\n",
    "- Basic Operations\n",
    "- Saving shape data\n",
    "- Get Polygon Centroids\n",
    "- Working with Points and Polygons\n",
    "-- Map Points and Polygons\n",
    "-- Get Points in Polygons\n",
    "\n",
    "**Input(s):** \n",
    "- Dataset (points/ bounds) url\n",
    "- Points/ bounds geometry column(s)\n",
    "- Points/ bounds crs's\n",
    "- Points/ bounds mapping color(s)\n",
    "- New filename\n",
    "\n",
    "**Output:** File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1pl9cGzD4pf"
   },
   "source": [
    "This function will handle common geo spatial exploratory methods. It covers everything discussed in the basic operations and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Slrp_tfD4pf"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "#\n",
    "# Work With Geometry Data\n",
    "# Description: geomSummary, getPointsInPolygons, mapPointsInPolygons, getCentroids\n",
    "#\n",
    "#\n",
    "def workWithGeometryData(method=False, df=False, polys=False, ptsCoordCol=False, polygonsCoordCol=False, polyColorCol=False, polygonsLabel='polyOnPoint', pntsClr='red', polysClr='white', interactive=False):\n",
    "\n",
    "  def geomSummary(df): return type(df), df.crs, df.columns;\n",
    "\n",
    "  def getCentroid(df, col): return df[col].representative_point()\n",
    "  #final['centroid'] = final['geometry'].centroid\n",
    "\n",
    "  # To 'import' a script you wrote, map its filepath into the sys\n",
    "  def getPolygonOnPoints(pts, polygons, ptsCoordCol, polygonsCoordCol, polygonsLabel, interactive):\n",
    "      count = 0\n",
    "      # We're going to keep a list of how many points we find.\n",
    "      boundaries = []\n",
    "\n",
    "      # Loop over polygons with index i.\n",
    "      for i, pt in pts.iterrows():\n",
    "          # print('Searching for point within Geom:', pt )\n",
    "          # Only one Label is accepted.\n",
    "          poly_on_this_point = 'false'\n",
    "          # Now loop over all polygons with index j.\n",
    "          for j, poly in polygons.iterrows():\n",
    "              if poly[polygonsCoordCol].contains(pt[ptsCoordCol]):\n",
    "                  # Then it's a hit! Add it to the list\n",
    "                  poly_on_this_point = poly[polygonsLabel]\n",
    "                  count = count + 1\n",
    "                  # pts = pts.drop([j])\n",
    "\n",
    "          # We could do all sorts, like grab a property of the\n",
    "          # points, but let's just append the number of them.\n",
    "          boundaries.append(poly_on_this_point)\n",
    "          clear_output(wait=True)\n",
    "\n",
    "      # Add the number of points for each poly to the dataframe.\n",
    "      pts = pts.assign(CSA2010 = boundaries)\n",
    "      if (interactive):\n",
    "        print( 'Total Points: ', (pts.size / len(pts.columns) ) )\n",
    "        print( 'Total Points in Polygons: ', count )\n",
    "        print( 'Prcnt Points in Polygons: ', count / (pts.size / len(pts.columns) ) )\n",
    "      return pts\n",
    "\n",
    "  # To 'import' a script you wrote, map its filepath into the sys\n",
    "  def getPointsInPolygons(pts, polygons, ptsCoordCol, polygonsCoordCol, interactive):\n",
    "    count = 0\n",
    "    total = pts.size / len(pts.columns)\n",
    "    # We're going to keep a list of how many points we find.\n",
    "    pts_in_polys = []\n",
    "\n",
    "    # Loop over polygons with index i.\n",
    "    for i, poly in polygons.iterrows():\n",
    "        # print('Searching for point within Geom:', poly )\n",
    "        # Keep a list of points in this poly\n",
    "        pts_in_this_poly = []\n",
    "\n",
    "        # Now loop over all points with index j.\n",
    "        for j, pt in pts.iterrows():\n",
    "            if poly[polygonsCoordCol].contains(pt[ptsCoordCol]):\n",
    "                # Then it's a hit! Add it to the list,\n",
    "                # and drop it so we have less hunting.\n",
    "                pts_in_this_poly.append(pt[ptsCoordCol])\n",
    "                pts = pts.drop([j])\n",
    "\n",
    "        # We could do all sorts, like grab a property of the\n",
    "        # points, but let's just append the number of them.\n",
    "        pts_in_polys.append(len(pts_in_this_poly))\n",
    "        if (interactive): print('Found this many points within the Geom:', len(pts_in_this_poly) )\n",
    "        count = count + len(pts_in_this_poly)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    # Add the number of points for each poly to the dataframe.\n",
    "    polygons['pointsinpolygon'] = gpd.GeoSeries(pts_in_polys)\n",
    "    if (interactive):\n",
    "      print( 'Total Points: ', total )\n",
    "      print( 'Total Points in Polygons: ', count )\n",
    "      print( 'Prcnt Points in Polygons: ', count / total )\n",
    "    return polygons\n",
    "\n",
    "  def mapPointsandPolygons(pnts, polys, pntsCl, polysClr, polyColorCol):\n",
    "    print('mapPointsandPolygons');\n",
    "    # We restrict to South America.\n",
    "    ax = 1\n",
    "    if polyColorCol:\n",
    "      ax = polys.plot( column=polyColorCol, legend=True)\n",
    "    else:\n",
    "      ax = polys.plot( color=polysClr, edgecolor='black')\n",
    "\n",
    "    # We can now plot our ``GeoDataFrame``.\n",
    "    pnts.plot(ax=ax, color=pntsClr)\n",
    "\n",
    "    return plt.show()\n",
    "\n",
    "\n",
    "  if method=='summary': return geomSummary(df);\n",
    "  if method=='ponp': return getPolygonOnPoints(df, polys, ptsCoordCol, polygonsCoordCol, polygonsLabel, interactive);\n",
    "  if method=='pinp': return getPointsInPolygons(df, polys, ptsCoordCol, polygonsCoordCol, interactive);\n",
    "  if method=='pandp': return mapPointsandPolygons(df, polys, pntsClr, polysClr, polyColorCol);\n",
    "  if method=='centroid': return getCentroid(df, col);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeWaDiIqD4pf"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def map_points(df, lat_col='latitude', lon_col='longitude', zoom_start=11, plot_points=False, cluster_points=False,\n",
    "               pt_radius=15, draw_heatmap=False, heat_map_weights_col=None, heat_map_weights_normalize=True,\n",
    "               heat_map_radius=15, popup=False):\n",
    "    \"\"\"Creates a map given a dataframe of points. Can also produce a heatmap overlay\n",
    "\n",
    "    Arg:\n",
    "        df: dataframe containing points to maps\n",
    "        lat_col: Column containing latitude (string)\n",
    "        lon_col: Column containing longitude (string)\n",
    "        zoom_start: Integer representing the initial zoom of the map\n",
    "        plot_points: Add points to map (boolean)\n",
    "        pt_radius: Size of each point\n",
    "        draw_heatmap: Add heatmap to map (boolean)\n",
    "        heat_map_weights_col: Column containing heatmap weights\n",
    "        heat_map_weights_normalize: Normalize heatmap weights (boolean)\n",
    "        heat_map_radius: Size of heatmap point\n",
    "\n",
    "    Returns:\n",
    "        folium map object\n",
    "    \"\"\"\n",
    "\n",
    "    ## center map in the middle of points center in\n",
    "    middle_lat = df[lat_col].median()\n",
    "    middle_lon = df[lon_col].median()\n",
    "\n",
    "    curr_map = folium.Map(location=[middle_lat, middle_lon], zoom_start=zoom_start)\n",
    "\n",
    "    # add points to map\n",
    "    if plot_points:\n",
    "      for _, row in df.iterrows():\n",
    "        # print([row[lat_col], row[lon_col]], row[popup])\n",
    "        folium.CircleMarker([row[lat_col], row[lon_col]],\n",
    "          radius=pt_radius,\n",
    "          popup=row[popup],\n",
    "          fill_color=\"#3db7e4\", # divvy color\n",
    "        ).add_to(curr_map)\n",
    "    if cluster_points:\n",
    "      marker_cluster = MarkerCluster().add_to(curr_map)\n",
    "      for index, row in df.iterrows():\n",
    "        folium.Marker( location=[row[lat_col],row[lon_col]], popup=row[popup], icon=None ).add_to(marker_cluster)\n",
    "\n",
    "    # add heatmap\n",
    "    if draw_heatmap:\n",
    "        # convert to (n, 2) or (n, 3) matrix format\n",
    "        if heat_map_weights_col is None:\n",
    "            cols_to_pull = [lat_col, lon_col]\n",
    "        else:\n",
    "            # if we have to normalize\n",
    "            if heat_map_weights_normalize:\n",
    "                df[heat_map_weights_col] = \\\n",
    "                    df[heat_map_weights_col] / df[heat_map_weights_col].sum()\n",
    "\n",
    "            cols_to_pull = [lat_col, lon_col, heat_map_weights_col]\n",
    "\n",
    "        stations = df[cols_to_pull].to_numpy()\n",
    "        curr_map.add_children(plugins.HeatMap(stations, radius=heat_map_radius))\n",
    "\n",
    "    return curr_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6IRfLBeD4pg"
   },
   "source": [
    "Processing Geometry is tedius enough to merit its own handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoPoh_JpD4pg"
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def readInGeometryData(url=False, porg=False, geom=False, lat=False, lng=False, revgeocode=False,  save=False, in_crs=4326, out_crs=False):\n",
    "\n",
    "  def reverseGeoCode(df, lat ):\n",
    "    # STREET\tCITY\tSTATE ZIP NAME\n",
    "    # , format_string=\"%s, BALTIMORE MD\"\n",
    "    geometry = []\n",
    "    geolocator = Nominatim(user_agent=\"my-application\")\n",
    "    for index, row in df.iterrows():\n",
    "      try:\n",
    "          geol = geolocator.geocode(row[lat], timeout=None)\n",
    "          pnt = Point(geol.longitude, geol.latitude)\n",
    "          geometry.append(pnt)\n",
    "      except:\n",
    "          geometry.append(Point(-76, 39) )\n",
    "          print(row[lat]);\n",
    "    return geometry\n",
    "\n",
    "\n",
    "\n",
    "  def readFile(url, geom, lat, lng, revgeocode, in_crs, out_crs):\n",
    "    df = False\n",
    "    gdf = False\n",
    "    ext = isinstance(url, pd.DataFrame)\n",
    "    if ext: ext='csv'\n",
    "    else: ext = url[-3:]\n",
    "\n",
    "    #XLS\n",
    "    # b16 = pd.read_excel('Jones.BirthsbyCensus2016.XLS', sheetname='Births')\n",
    "\n",
    "    # The file extension is used to determine the appropriate import method.\n",
    "    if ext in ['son', 'kml', 'shp', 'pgeojson']: gdf = gpd.read_file(url)\n",
    "    if ext == 'csv':\n",
    "      df = url if isinstance(url, pd.DataFrame) else pd.read_csv(url)\n",
    "      # Read using Geom, Lat, Lat/Lng, revGeoCode\n",
    "      if revgeocode=='y': df['geometry'] = reverseGeoCode(df, lat)\n",
    "      elif geom: df['geometry'] = df[geom].apply(lambda x: loads( str(x) ))\n",
    "      elif lat==lng: df['geometry'] = df[lat].apply(lambda x: loads( str(x) ))\n",
    "      elif lat!=lng: df['geometry'] = gpd.points_from_xy(df[lng], df[lat]);\n",
    "\n",
    "      gdf = GeoDataFrame(df, crs=in_crs, geometry='geometry') #crs=2248\n",
    "      if not out_crs == in_crs: gdf = gdf.to_crs(epsg=out_crs)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "  def getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs):\n",
    "    addr=False\n",
    "\n",
    "    if not url: url = input(\"Please enter the location of your dataset: \" )\n",
    "    # if url[-3:] == 'csv' :\n",
    "    #   df = pd.read_csv(url,index_col=0,nrows=1)\n",
    "    #   print(df.columns)\n",
    "\n",
    "    # Geometries inside\n",
    "    if geom and not (lat and lng): porg = 'g'\n",
    "    # Point data inside\n",
    "    elif not geom and lat or lng:\n",
    "      porg = 'p';\n",
    "      if not lat: lat = lng\n",
    "      if not lng: lng = lat\n",
    "\n",
    "    # If the P/G could not be infered...\n",
    "    if not (porg in ['p', 'g']):\n",
    "      if not revgeocode in ['y', 'n']: revgeocode = input(\"Do your records need reverse geocoding: (Enter: y/n') \" )\n",
    "      if revgeocode == 'y': porg = 'p'; lng = lat = input(\"Please enter the column name where the address is stored: \" );\n",
    "      elif revgeocode == 'n': porg = input(\"\"\"Do the records in this dataset use (P)oints or (g)eometric polygons?: (Enter: 'p' or 'g') \"\"\" );\n",
    "      else: return getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs);\n",
    "\n",
    "      if porg=='p':\n",
    "          if not lat: lat = input(\"Please enter the column name where the latitude coordinate is stored: \" );\n",
    "          if not lng: lng = input(\"Please enter the column name where the longitude cooridnate is stored: (Could be same as the lat) \" );\n",
    "      elif porg=='g':\n",
    "        if not geom: geom = input(\"Please enter column name where the geometry data is stored: (*optional, skip if unkown)\" );\n",
    "      else: return getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs)\n",
    "\n",
    "    if not out_crs: out_crs=in_crs\n",
    "\n",
    "    return url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs\n",
    "\n",
    "  # This function uses all the other functions\n",
    "  def main(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs):\n",
    "\n",
    "    # Check for missing values. retrieve them\n",
    "    if (isinstance(url, pd.DataFrame)): print('Converting DF to GDF')\n",
    "    elif (not (url and porg) ) or (\n",
    "        not (porg == 'p' or porg == 'g') ) or (\n",
    "        porg == 'g' and not geom) or (\n",
    "        porg == 'p' and (not (lat and lng) ) ):\n",
    "      return readInGeometryData( *getGeoParams(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs) );\n",
    "\n",
    "    # print(f\"RECIEVED url: {url}, \\r\\n porg: {porg}, \\r\\n geom: {geom}, \\r\\n lat: {lat}, \\r\\n lng: {lng}, \\r\\n revgeocode: {revgeocode}, \\r\\n in_crs: {in_crs}, \\r\\n out_crs: {out_crs}\")\n",
    "\n",
    "    # Quit if the Columns dont exist -> CSV Only\n",
    "    # status = checkColumns(url, geom, lat, lng)\n",
    "    # if status == False: print('A specified column does not exist'); return False;\n",
    "\n",
    "    # Perform operation\n",
    "    gdf = readFile(url, geom, lat, lng, revgeocode, in_crs, out_crs)\n",
    "\n",
    "    # Tidy up\n",
    "\n",
    "    # Save\n",
    "    # if save: saveGeoData(gdf, url, fileName, driver='esri')\n",
    "\n",
    "    return gdf\n",
    "\n",
    "  return main(url, porg, geom, lat, lng, revgeocode, save, in_crs, out_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVpNDE4QD4pg"
   },
   "source": [
    "As you can see we have a lot of points. \n",
    "Lets see if there is any better way to visualize this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "704Y-OsrD4pg"
   },
   "source": [
    "## Example: Using the advanced Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRJ16sV8D4pg"
   },
   "source": [
    "### Playing with Points: Geoloom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "so-CECA8D4pg"
   },
   "source": [
    "#### Points In Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4fKcwsED4pg"
   },
   "source": [
    "The red dots from when we mapped the geoloom points above were a bit too noisy. \n",
    "\n",
    "Lets create a choropleth instead!\n",
    "\n",
    "We can do this by [aggregating](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html) by CSA.\n",
    "\n",
    "To do this, start of by finding which points are inside of which polygons!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgyKSbFUD4pg"
   },
   "source": [
    "Since the geoloom data does not have a CSA dataset, we will need merge it to one that does!\r\n",
    "\r\n",
    "Lets use the childhood poverty link from example one and load it up because it contains the geometry data and the csa labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP20QZeBD4ph"
   },
   "outputs": [],
   "source": [
    "# This dataset is taken from the public database provided by BNIAJFI hosted by Esri / ArcGIS\r\n",
    "# BNIA ArcGIS Homepage: https://data-bniajfi.opendata.arcgis.com/\r\n",
    "csa_gdf_url = \"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Hhchpov/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\"\r\n",
    "csa_gdf_url = readInGeometryData(url=csa_gdf_url, porg=False, geom='geometry', lat=False, lng=False, revgeocode=False,  save=False, in_crs=2248, out_crs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h54TJxGQD4ph"
   },
   "source": [
    "And now lets pull in our geoloom data. But to be sure, drop the empty geometry columns or the function directly below will now work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4rDqq4KD4ph"
   },
   "outputs": [],
   "source": [
    "geoloom_gdf_url = \"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Geoloom_Crowd/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\"\r\n",
    "geoloom_gdf = readInGeometryData(url=geoloom_gdf_url, porg=False, geom='geometry', lat=False, lng=False, revgeocode=False,  save=False, in_crs=4326, out_crs=False)\r\n",
    "geoloom_gdf = geoloom_gdf.dropna(subset=['geometry'])\r\n",
    "# geoloom_gdf = geoloom_gdf.drop(columns=['POINT_X','POINT_Y'])\r\n",
    "geoloom_gdf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxsWjMmTD4ph"
   },
   "source": [
    "And now use a point in polygon method 'ponp' to get the CSA2010 column from our CSA dataset added as a column to each geoloom record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9E80S-6hD4ph"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas = workWithGeometryData(method='pinp', df=geoloom_gdf, polys=csa_gdf, ptsCoordCol='geometry', polygonsCoordCol='geometry', polyColorCol='hhchpov18', polygonsLabel='CSA2010', pntsClr='red', polysClr='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hyso-uCUD4pj"
   },
   "source": [
    "You'll see you have a 'pointsinpolygons' column now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jj-8yQ4xD4pj"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas[13:].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqcxbK3rD4pj"
   },
   "outputs": [],
   "source": [
    "# In order for this choropleth to work, the total number of banks in each csa must be tallied.\n",
    "# This can be done programmatically, but i havent added the code. \n",
    "# The column needs to be changed from CSA to whatever this new tallied column is named.\n",
    "geoloom_w_csas.plot( column='pointsinpolygon', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNHCs1-BD4pj"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtOszwjVD4pj"
   },
   "source": [
    "#### Polygons in Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5xtUh70D4pk"
   },
   "source": [
    "Alternately, you can run the ponp function and have returned the geoloom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJK3wLqeD4pk"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas = workWithGeometryData(method='ponp', df=geoloom_gdf, polys=csa_gdf, ptsCoordCol='geometry', polygonsCoordCol='geometry', polyColorCol='hhchpov18', polygonsLabel='CSA2010', pntsClr='red', polysClr='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3t-ZlVyD4pk"
   },
   "source": [
    "We can count the totals per CSA using `value_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oobkio7dD4pk"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas['POINT_Y'] = geoloom_w_csas.centroid.y\r\n",
    "geoloom_w_csas['POINT_X'] = geoloom_w_csas.centroid.x\r\n",
    "geoloom_w_csas.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvpW90PlD4pk"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas['CSA2010'].value_counts(normalize=False, sort=True, ascending=False, bins=None, dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgfhjiC5D4pk"
   },
   "source": [
    "Alternately, we could map the centroid of boundaries within another boundary to find boundaries within boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kiQaNOLlD4pl"
   },
   "outputs": [],
   "source": [
    "# We already know the x and y columns because we just saved them as such.\r\n",
    "geoloom_w_csas['POINT_X'] = pd.to_numeric(geoloom_w_csas['POINT_X'], errors='coerce')\r\n",
    "geoloom_w_csas['POINT_Y'] = pd.to_numeric(geoloom_w_csas['POINT_Y'], errors='coerce')\r\n",
    "# df = df.replace(np.nan, 0, regex=True)\r\n",
    "\r\n",
    "# And filter out for points only in Baltimore City. \r\n",
    "geoloom_w_csas = geoloom_w_csas[ geoloom_w_csas['POINT_Y'] > 39.3  ]\r\n",
    "geoloom_w_csas = geoloom_w_csas[ geoloom_w_csas['POINT_Y'] < 39.5  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDVBm3AKD4pl"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas = geoloom_w_csas.dropna(subset=['POINT_X', 'POINT_Y'])\r\n",
    "map_points(geoloom_w_csas, lat_col='POINT_Y', lon_col='POINT_X', zoom_start=11, plot_points=True, cluster_points=False, \r\n",
    "           pt_radius=15, draw_heatmap=True, heat_map_weights_col='POINT_X', heat_map_weights_normalize=True, \r\n",
    "           heat_map_radius=15, popup='CSA2010')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBY2VFqJD4pl"
   },
   "source": [
    "But if that doesn't do it for you, we can also create heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3CgCznKD4pl"
   },
   "outputs": [],
   "source": [
    "!apt install libspatialindex-dev\r\n",
    "!pip install rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFwi9bNAD4pl"
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import folium\r\n",
    "import geopandas as gpd\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from branca.colormap import linear\r\n",
    "from folium.plugins import TimeSliderChoropleth\r\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ujEcjOND4pl"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFs6-FwDD4pm"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ql3ysEUCD4pm"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GuZV6bVD4pm"
   },
   "outputs": [],
   "source": [
    "geoloom_w_csas['POINT_Y'] = geoloom_w_csas.centroid.y\r\n",
    "geoloom_w_csas['POINT_X'] = geoloom_w_csas.centroid.x\r\n",
    "\r\n",
    "# We already know the x and y columns because we just saved them as such.\r\n",
    "geoloom_w_csas['POINT_X'] = pd.to_numeric(geoloom_w_csas['POINT_X'], errors='coerce')\r\n",
    "geoloom_w_csas['POINT_Y'] = pd.to_numeric(geoloom_w_csas['POINT_Y'], errors='coerce')\r\n",
    "# df = df.replace(np.nan, 0, regex=True)\r\n",
    "\r\n",
    "# And filter out for points only in Baltimore City. \r\n",
    "geoloom_w_csas = geoloom_w_csas[ geoloom_w_csas['POINT_Y'] > 39.3  ]\r\n",
    "geoloom_w_csas = geoloom_w_csas[ geoloom_w_csas['POINT_Y'] < 39.5  ]\r\n",
    "\r\n",
    "map_points(geoloom_w_csas, lat_col='POINT_Y', lon_col='POINT_X', zoom_start=11, plot_points=True, cluster_points=True, \r\n",
    "           pt_radius=15, draw_heatmap=True, heat_map_weights_col='POINT_X', heat_map_weights_normalize=True, \r\n",
    "           heat_map_radius=15, popup='CSA2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7RgoF0VD4pm"
   },
   "outputs": [],
   "source": [
    "\r\n",
    "geoloom_w_csas.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noEpKgprD4pm"
   },
   "outputs": [],
   "source": [
    "# MarkerCluster.ipynb\n",
    "# https://github.com/python-visualization/folium/blob/master/examples/MarkerCluster.ipynb\n",
    "m = folium.Map(location=[39.28759453969165, -76.61278931706487], zoom_start=12)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "stations = geoloom_w_csas.apply(lambda p: folium.Marker( location=[p.y,p.x], popup='Add popup text here.', icon=None ).add_to(marker_cluster) )\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDkDTFCkD4pm"
   },
   "source": [
    "And Time Sliders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgJcG-zBD4pm"
   },
   "source": [
    "##### Choropleth Timeslider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IIDfCadD4pn"
   },
   "source": [
    "https://github.com/python-visualization/folium/blob/master/examples/TimeSliderChoropleth.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t58uggQND4pn"
   },
   "source": [
    "To simulate that data is sampled at different times we random sample data for n_periods rows of data. __Note__ that the geodata and random sampled data is linked through the feature_id, which is the index of the GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGcVwJRlD4pn"
   },
   "outputs": [],
   "source": [
    "periods = 10\n",
    "datetime_index = pd.date_range('2010', periods=periods, freq='Y')\n",
    "dt_index_epochs = ( datetime_index.astype(int) ).astype('U10')\n",
    "datetime_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNyZan1MD4pn"
   },
   "outputs": [],
   "source": [
    "# Style each boundry with randomness.\n",
    "styledata = {}\n",
    "for country in geoloom.index:\n",
    "    df = pd.DataFrame(\n",
    "        {'color': np.random.normal(size=periods),\n",
    "         'opacity':  [1,2,3,4,5,6,7,8,9,1] },\n",
    "        index=dt_index_epochs\n",
    "    )\n",
    "    df = df.cumsum()\n",
    "    styledata[country] = df\n",
    "ax = df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2oK5vkrD4pn"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN3mWMc6D4pn"
   },
   "source": [
    "We see that we generated two series of data for each country; one for color and one for opacity. Let's plot them to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnZX4R2dD4pn"
   },
   "outputs": [],
   "source": [
    "max_color, min_color, max_opacity, min_opacity = 0, 0, 0, 0\n",
    "for country, data in styledata.items():\n",
    "    max_color = max(max_color, data['color'].max())\n",
    "    min_color = min(max_color, data['color'].min())\n",
    "    max_opacity = max(max_color, data['opacity'].max())\n",
    "    max_opacity = min(max_color, data['opacity'].max())\n",
    "linear.PuRd_09.scale(min_color, max_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3NwI21ND4pn"
   },
   "source": [
    "We want to map the column named color to a hex color. To do this we use a normal colormap. To create the colormap, we calculate the maximum and minimum values over all the timeseries. We also need the max/min of the opacity column, so that we can map that column into a range [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErFGMwr1D4pn"
   },
   "outputs": [],
   "source": [
    "max_color, min_color, max_opacity, min_opacity = 0, 0, 0, 0\n",
    "for country, data in styledata.items():\n",
    "    max_color = max(max_color, data['color'].max())\n",
    "    min_color = min(max_color, data['color'].min())\n",
    "    max_opacity = max(max_color, data['opacity'].max())\n",
    "    max_opacity = min(max_color, data['opacity'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5mqs4p9D4pn"
   },
   "outputs": [],
   "source": [
    "from branca.colormap import linear\n",
    "cmap = linear.PuRd_09.scale(min_color, max_color)\n",
    "def norm(x): return (x - x.min()) / (x.max() - x.min())\n",
    "for country, data in styledata.items():\n",
    "    data['color'] = data['color'].apply(cmap)\n",
    "    data['opacity'] = norm(data['opacity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GznXWdgD4po"
   },
   "source": [
    "Finally we use pd.DataFrame.to_dict() to convert each dataframe into a dictionary, and place each of these in a map from country id to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZiQdurlD4po"
   },
   "outputs": [],
   "source": [
    "from folium.plugins import TimeSliderChoropleth\n",
    "m = folium.Map([39.28759453969165, -76.61278931706487], zoom_start=12)\n",
    "g = TimeSliderChoropleth(\n",
    "    geoloom.to_json(),\n",
    "    styledict={\n",
    "      str(country): data.to_dict(orient='index') for\n",
    "      country, data in styledata.items()\n",
    "    }\n",
    ").add_to(m)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_Map_Basics_Intake_and_Operations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
