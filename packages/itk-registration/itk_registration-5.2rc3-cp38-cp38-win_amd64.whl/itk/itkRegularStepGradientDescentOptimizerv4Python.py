# This file was automatically generated by SWIG (http://www.swig.org).
# Version 4.0.2
#
# Do not make changes to this file unless you know what you are doing--modify
# the SWIG interface file instead.


import collections

from sys import version_info as _version_info
if _version_info < (3, 6, 0):
    raise RuntimeError("Python 3.6 or later required")


from . import _ITKOptimizersv4Python



from sys import version_info as _swig_python_version_info
if _swig_python_version_info < (2, 7, 0):
    raise RuntimeError("Python 2.7 or later required")

# Import the low-level C/C++ module
if __package__ or "." in __name__:
    from . import _itkRegularStepGradientDescentOptimizerv4Python
else:
    import _itkRegularStepGradientDescentOptimizerv4Python

try:
    import builtins as __builtin__
except ImportError:
    import __builtin__

_swig_new_instance_method = _itkRegularStepGradientDescentOptimizerv4Python.SWIG_PyInstanceMethod_New
_swig_new_static_method = _itkRegularStepGradientDescentOptimizerv4Python.SWIG_PyStaticMethod_New

def _swig_repr(self):
    try:
        strthis = "proxy of " + self.this.__repr__()
    except __builtin__.Exception:
        strthis = ""
    return "<%s.%s; %s >" % (self.__class__.__module__, self.__class__.__name__, strthis,)


def _swig_setattr_nondynamic_instance_variable(set):
    def set_instance_attr(self, name, value):
        if name == "thisown":
            self.this.own(value)
        elif name == "this":
            set(self, name, value)
        elif hasattr(self, name) and isinstance(getattr(type(self), name), property):
            set(self, name, value)
        else:
            raise AttributeError("You cannot add instance attributes to %s" % self)
    return set_instance_attr


def _swig_setattr_nondynamic_class_variable(set):
    def set_class_attr(cls, name, value):
        if hasattr(cls, name) and not isinstance(getattr(cls, name), property):
            set(cls, name, value)
        else:
            raise AttributeError("You cannot add class attributes to %s" % cls)
    return set_class_attr


def _swig_add_metaclass(metaclass):
    """Class decorator for adding a metaclass to a SWIG wrapped class - a slimmed down version of six.add_metaclass"""
    def wrapper(cls):
        return metaclass(cls.__name__, cls.__bases__, cls.__dict__.copy())
    return wrapper


class _SwigNonDynamicMeta(type):
    """Meta class to enforce nondynamic attributes (no new attributes) for a class"""
    __setattr__ = _swig_setattr_nondynamic_class_variable(type.__setattr__)


import collections.abc
import itk.itkGradientDescentOptimizerv4Python
import itk.itkGradientDescentOptimizerBasev4Python
import itk.itkIndexPython
import itk.itkSizePython
import itk.pyBasePython
import itk.itkOffsetPython
import itk.ITKCommonBasePython
import itk.itkObjectToObjectOptimizerBasePython
import itk.itkOptimizerParameterScalesEstimatorPython
import itk.itkOptimizerParametersPython
import itk.vnl_vectorPython
import itk.vnl_matrixPython
import itk.stdcomplexPython
import itk.itkArrayPython
import itk.itkObjectToObjectMetricBasePython
import itk.itkSingleValuedCostFunctionv4Python
import itk.itkCostFunctionPython

def itkRegularStepGradientDescentOptimizerv4D_New():
    return itkRegularStepGradientDescentOptimizerv4D.New()

class itkRegularStepGradientDescentOptimizerv4D(itk.itkGradientDescentOptimizerv4Python.itkGradientDescentOptimizerv4TemplateD):
    r"""


    Regular Step Gradient descent optimizer.

    This optimizer is a variant of gradient descent that attempts to
    prevent it from taking steps that are too large. At each iteration,
    this optimizer will take a step along the direction of the metric
    derivative. Each time the direction of the derivative abruptly
    changes, the optimizer assumes that a local extrema has been passed
    and reacts by reducing the step length by a relaxation factor that is
    set to 0.5 by default. The default value for the initial step length
    is 1, and this value can only be changed manually via
    SetLearningRate() since this optimizer does not use the ScaleEstimator
    to automatically estimate the learning rate. Also note that unlike the
    previous version of ReuglarStepGradientDescentOptimizer, ITKv4 does
    not have a "maximize/minimize" option to modify the effect of the
    metric derivative. The assigned metric is assumed to return a
    parameter derivative result that "improves" the optimization. 
    """

    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc="The membership flag")

    def __init__(self, *args, **kwargs):
        raise AttributeError("No constructor defined")
    __repr__ = _swig_repr
    __New_orig__ = _swig_new_static_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D___New_orig__)
    Clone = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_Clone)
    SetMinimumStepLength = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_SetMinimumStepLength)
    GetMinimumStepLength = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_GetMinimumStepLength)
    SetRelaxationFactor = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_SetRelaxationFactor)
    GetRelaxationFactor = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_GetRelaxationFactor)
    SetGradientMagnitudeTolerance = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_SetGradientMagnitudeTolerance)
    GetGradientMagnitudeTolerance = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_GetGradientMagnitudeTolerance)
    SetCurrentLearningRateRelaxation = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_SetCurrentLearningRateRelaxation)
    GetCurrentLearningRateRelaxation = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_GetCurrentLearningRateRelaxation)
    StartOptimization = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_StartOptimization)
    GetCurrentStepLength = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_GetCurrentStepLength)
    __swig_destroy__ = _itkRegularStepGradientDescentOptimizerv4Python.delete_itkRegularStepGradientDescentOptimizerv4D
    cast = _swig_new_static_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_cast)

    def New(*args, **kargs):
        """New() -> itkRegularStepGradientDescentOptimizerv4D

        Create a new object of the class itkRegularStepGradientDescentOptimizerv4D and set the input and the parameters if some
        named or non-named arguments are passed to that method.

        New() tries to assign all the non named parameters to the input of the new objects - the
        first non named parameter in the first input, etc.

        The named parameters are used by calling the method with the same name prefixed by 'Set'.

        Ex:

          itkRegularStepGradientDescentOptimizerv4D.New(reader, threshold=10)

        is (most of the time) equivalent to:

          obj = itkRegularStepGradientDescentOptimizerv4D.New()
          obj.SetInput(0, reader.GetOutput())
          obj.SetThreshold(10)
        """
        obj = itkRegularStepGradientDescentOptimizerv4D.__New_orig__()
        from itk.support import itkTemplate
        itkTemplate.New(obj, *args, **kargs)
        return obj
    New = staticmethod(New)


# Register itkRegularStepGradientDescentOptimizerv4D in _itkRegularStepGradientDescentOptimizerv4Python:
_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_swigregister(itkRegularStepGradientDescentOptimizerv4D)
itkRegularStepGradientDescentOptimizerv4D___New_orig__ = _itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D___New_orig__
itkRegularStepGradientDescentOptimizerv4D_cast = _itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4D_cast


def itkRegularStepGradientDescentOptimizerv4F_New():
    return itkRegularStepGradientDescentOptimizerv4F.New()

class itkRegularStepGradientDescentOptimizerv4F(itk.itkGradientDescentOptimizerv4Python.itkGradientDescentOptimizerv4TemplateF):
    r"""


    Regular Step Gradient descent optimizer.

    This optimizer is a variant of gradient descent that attempts to
    prevent it from taking steps that are too large. At each iteration,
    this optimizer will take a step along the direction of the metric
    derivative. Each time the direction of the derivative abruptly
    changes, the optimizer assumes that a local extrema has been passed
    and reacts by reducing the step length by a relaxation factor that is
    set to 0.5 by default. The default value for the initial step length
    is 1, and this value can only be changed manually via
    SetLearningRate() since this optimizer does not use the ScaleEstimator
    to automatically estimate the learning rate. Also note that unlike the
    previous version of ReuglarStepGradientDescentOptimizer, ITKv4 does
    not have a "maximize/minimize" option to modify the effect of the
    metric derivative. The assigned metric is assumed to return a
    parameter derivative result that "improves" the optimization. 
    """

    thisown = property(lambda x: x.this.own(), lambda x, v: x.this.own(v), doc="The membership flag")

    def __init__(self, *args, **kwargs):
        raise AttributeError("No constructor defined")
    __repr__ = _swig_repr
    __New_orig__ = _swig_new_static_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F___New_orig__)
    Clone = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_Clone)
    SetMinimumStepLength = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_SetMinimumStepLength)
    GetMinimumStepLength = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_GetMinimumStepLength)
    SetRelaxationFactor = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_SetRelaxationFactor)
    GetRelaxationFactor = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_GetRelaxationFactor)
    SetGradientMagnitudeTolerance = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_SetGradientMagnitudeTolerance)
    GetGradientMagnitudeTolerance = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_GetGradientMagnitudeTolerance)
    SetCurrentLearningRateRelaxation = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_SetCurrentLearningRateRelaxation)
    GetCurrentLearningRateRelaxation = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_GetCurrentLearningRateRelaxation)
    StartOptimization = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_StartOptimization)
    GetCurrentStepLength = _swig_new_instance_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_GetCurrentStepLength)
    __swig_destroy__ = _itkRegularStepGradientDescentOptimizerv4Python.delete_itkRegularStepGradientDescentOptimizerv4F
    cast = _swig_new_static_method(_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_cast)

    def New(*args, **kargs):
        """New() -> itkRegularStepGradientDescentOptimizerv4F

        Create a new object of the class itkRegularStepGradientDescentOptimizerv4F and set the input and the parameters if some
        named or non-named arguments are passed to that method.

        New() tries to assign all the non named parameters to the input of the new objects - the
        first non named parameter in the first input, etc.

        The named parameters are used by calling the method with the same name prefixed by 'Set'.

        Ex:

          itkRegularStepGradientDescentOptimizerv4F.New(reader, threshold=10)

        is (most of the time) equivalent to:

          obj = itkRegularStepGradientDescentOptimizerv4F.New()
          obj.SetInput(0, reader.GetOutput())
          obj.SetThreshold(10)
        """
        obj = itkRegularStepGradientDescentOptimizerv4F.__New_orig__()
        from itk.support import itkTemplate
        itkTemplate.New(obj, *args, **kargs)
        return obj
    New = staticmethod(New)


# Register itkRegularStepGradientDescentOptimizerv4F in _itkRegularStepGradientDescentOptimizerv4Python:
_itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_swigregister(itkRegularStepGradientDescentOptimizerv4F)
itkRegularStepGradientDescentOptimizerv4F___New_orig__ = _itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F___New_orig__
itkRegularStepGradientDescentOptimizerv4F_cast = _itkRegularStepGradientDescentOptimizerv4Python.itkRegularStepGradientDescentOptimizerv4F_cast



