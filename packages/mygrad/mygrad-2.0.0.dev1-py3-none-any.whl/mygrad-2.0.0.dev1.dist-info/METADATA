Metadata-Version: 2.1
Name: mygrad
Version: 2.0.0.dev1
Summary: A sleek auto-differentiation library that wraps numpy.
Home-page: https://github.com/rsokl/MyGrad
Author: Ryan Soklaski
Author-email: rsoklaski@gmail.com
License: MIT
Download-URL: https://github.com/rsokl/mygrad/tarball/2.0.0.dev1
Platform: UNKNOWN
Classifier: Development Status :: 5 - Production/Stable
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Education
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Topic :: Scientific/Engineering
Requires-Python: >=3.7
Requires-Dist: numpy (>=1.17)
Provides-Extra: rnn
Requires-Dist: numba (>=0.34.0) ; extra == 'rnn'


mygrad is a simple, NumPy-centric autograd library. An autograd library enables
you to automatically compute derivatives of mathematical functions. This library is
designed to serve primarily as an education tool for learning about gradient-based
machine learning; it is easy to install, has a readable and easily customizable code base,
and provides a sleek interface that mimics NumPy. Furthermore, it leverages NumPy's
vectorization to achieve good performance despite the library's simplicity.

This is not meant to be a competitor to libraries like PyTorch (which mygrad most
closely resembles) or TensorFlow. Rather, it is meant to serve as a useful tool for
students who are learning about training neural networks using back propagation.


